#!/usr/bin/env python3
"""
Enterprise NL2SQL Demo in a Single File
========================================

A refactored, comprehensive demonstration of a Text-to-SQL pipeline 
showcasing modular design within a single script. It handles a complex, 
multi-table join BI scenario.

Architecture Highlights:
1.  **Config-Driven**: A central `CONFIG` dictionary manages all settings.
2.  **Modular Classes**: Internal classes (`DBManager`, `VectorStore`, `LLMProvider`) 
    encapsulate specific responsibilities.
3.  **Orchestrator Pattern**: The main `NL2SQLPipeline` class coordinates the workflow.
4.  **Complex BI Scenario**: Uses a 5-table schema for realistic sales analysis.

Author: Tyler (Refactored by Gemini)
License: MIT
"""

import os
import sqlite3
import json
import logging
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

# Conditional imports for LLM providers
try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

# --- Configuration Block ----------------------------------------------------
# All settings are managed here, replacing an external config file for simplicity.
CONFIG = {
    "database": {
        "path": "enterprise_bi.db",
    },
    "embedding_model": "text-embedding-v4",
    "llm": {
        "provider": "dashscope",  # or "openai"
        "api_key_env": "DASHSCOPE_API_KEY", # or "OPENAI_API_KEY"
        "models": {
            "sql_generation": "qwen-plus",
            "answer_generation": "qwen-plus",
        }
    },
    "prompts": {
        "sql_generation": """
ä½ æ˜¯ä¸€ä½SQLiteæ•°æ®åº“ä¸“å®¶ã€‚æ ¹æ®ç»™å®šçš„æ•°æ®åº“æ¨¡å¼å’Œè‡ªç„¶è¯­è¨€é—®é¢˜ï¼Œç”Ÿæˆå‡†ç¡®ä¸”å¯æ‰§è¡Œçš„SQLæŸ¥è¯¢ã€‚

**é‡è¦çº¦æŸ**ï¼š
1. åªèƒ½ä½¿ç”¨æä¾›çš„æ•°æ®åº“æ¨¡å¼ä¸­æ˜ç¡®å­˜åœ¨çš„è¡¨å’Œå­—æ®µ
2. å¦‚æœé—®é¢˜è¦æ±‚çš„æ•°æ®åœ¨ç»™å®šçš„DDLä¸­ä¸å­˜åœ¨ï¼Œå¿…é¡»æ‹’ç»ç”ŸæˆSQL
3. æ‹’ç»æ—¶è¿”å›ï¼šSCHEMA_INSUFFICIENT: [å…·ä½“è¯´æ˜ç¼ºå°‘ä»€ä¹ˆæ•°æ®]

### æ•°æ®åº“æ¨¡å¼:
{schema_context}

### é—®é¢˜:
{question}

### è¦æ±‚:
- å¦‚æœæ‰€éœ€å­—æ®µéƒ½å­˜åœ¨ï¼šè¿”å›çº¯SQLè¯­å¥ï¼Œä¸è¦æœ‰ä»»ä½•è§£é‡Šæˆ–markdownæ ¼å¼
- å¦‚æœç¼ºå°‘å¿…è¦å­—æ®µï¼šè¿”å› SCHEMA_INSUFFICIENT: [è¯´æ˜åŸå› ]

SQL:
""",
        "answer_generation": """
ä½ æ˜¯ä¸€ä½ä¸“ä¸šçš„å•†ä¸šæ™ºèƒ½åŠ©æ‰‹ã€‚åŸºäºç”¨æˆ·çš„é—®é¢˜ã€SQLæŸ¥è¯¢å’Œæ•°æ®ç»“æ„æ‘˜è¦ï¼Œæä¾›æœ‰ä»·å€¼çš„åˆ†æå›ç­”ã€‚

æ³¨æ„ï¼šå‡ºäºæ•°æ®å®‰å…¨è€ƒè™‘ï¼Œä½ æ”¶åˆ°çš„æ˜¯æ•°æ®ç»“æ„æ‘˜è¦è€Œéå®é™…æ•°æ®å€¼ã€‚è¯·åŸºäºæŸ¥è¯¢é€»è¾‘å’Œæ•°æ®ç»“æ„æä¾›ä¸“ä¸šåˆ†æã€‚

### ç”¨æˆ·é—®é¢˜:
{question}

### æ‰§è¡Œçš„SQLæŸ¥è¯¢:
{sql_query}

### æ•°æ®ç»“æ„æ‘˜è¦:
{data_summary}

### åˆ†æè¦æ±‚:
1. åŸºäºSQLæŸ¥è¯¢é€»è¾‘åˆ†æä¸šåŠ¡é—®é¢˜
2. è§£é‡ŠæŸ¥è¯¢æ¶‰åŠçš„ä¸šåŠ¡æŒ‡æ ‡å’Œå…³ç³»
3. æ ¹æ®æ•°æ®ç»“æ„æä¾›åˆç†çš„ä¸šåŠ¡æ´å¯Ÿ
4. æä¾›æ•°æ®é©±åŠ¨çš„å»ºè®®ï¼ˆå¦‚é€‚ç”¨ï¼‰

### ä¸“ä¸šåˆ†æ:
"""
    }
}

# --- Logging Setup ----------------------------------------------------------
# é…ç½®æ—¥å¿—åŒæ—¶è¾“å‡ºåˆ°æ§åˆ¶å°å’Œæ–‡ä»¶
log_format = '%(asctime)s - %(levelname)s - [%(name)s] %(message)s'

# åˆ›å»ºlogger
logger = logging.getLogger(__name__)
logger.setLevel(logging.INFO)

# æ¸…é™¤å·²æœ‰çš„handlersï¼ˆé¿å…é‡å¤ï¼‰
if logger.handlers:
    logger.handlers.clear()

# åˆ›å»ºæ§åˆ¶å°handler
console_handler = logging.StreamHandler()
console_handler.setLevel(logging.INFO)
console_formatter = logging.Formatter(log_format)
console_handler.setFormatter(console_formatter)

# åˆ›å»ºæ–‡ä»¶handler
file_handler = logging.FileHandler('nl2sql_demo_info.log', mode='w', encoding='utf-8')
file_handler.setLevel(logging.INFO)
file_formatter = logging.Formatter(log_format)
file_handler.setFormatter(file_formatter)

# æ·»åŠ handlersåˆ°logger
logger.addHandler(console_handler)
logger.addHandler(file_handler)

# --- Data Classes -----------------------------------------------------------
@dataclass
class TableSchema:
    """Represents a database table schema with metadata."""
    name: str
    ddl: str
    description: str
    
@dataclass
class QueryResult:
    """Represents the result of a SQL query execution."""
    success: bool
    data: List[Dict[str, Any]]
    sql: str
    error: Optional[str] = None

# --- Modular Components -----------------------------------------------------

class DBManager:
    """
    Manages all database interactions, including schema creation and querying.
    In a real app, this would handle connection pooling (e.g., with SQLAlchemy).
    """
    def __init__(self, db_config: Dict[str, Any]):
        self.db_path = db_config['path']
        logger.info(f"DBManager initialized for database: {self.db_path}")
        self._init_database()
    
    def _init_database(self):
        """Initializes the database and creates the 5-table enterprise schema if not present."""
        logger.info("Initializing database schema...")
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        
        # Check if tables already exist
        cursor.execute("SELECT name FROM sqlite_master WHERE type='table' AND name='sales'")
        if cursor.fetchone():
            logger.info("Database schema already exists. Skipping creation.")
            conn.close()
            return

        logger.info("Creating enterprise BI schema (5 tables)...")
        self._create_enterprise_schema(cursor)
        self._insert_sample_data(cursor)
        
        conn.commit()
        conn.close()
        logger.info("Database initialized successfully.")

    def _create_enterprise_schema(self, cursor: sqlite3.Cursor):
        """Defines and creates the 10-table schema with rich metadata for complex enterprise analysis."""
        # Drop existing tables
        tables_to_drop = ['sales', 'employees', 'departments', 'products', 'regions', 
                         'customers', 'orders', 'suppliers', 'inventory', 'promotions']
        for table in tables_to_drop:
            cursor.execute(f"DROP TABLE IF EXISTS {table};")

        # 1. éƒ¨é—¨è¡¨ - ç»„ç»‡æ¶æ„ä¿¡æ¯
        cursor.execute("""
        CREATE TABLE departments (
            id INTEGER PRIMARY KEY, 
            name TEXT NOT NULL UNIQUE, -- éƒ¨é—¨åç§°ï¼Œå¦‚ï¼šé”€å”®éƒ¨ã€æŠ€æœ¯éƒ¨ã€è´¢åŠ¡éƒ¨
            description TEXT, -- éƒ¨é—¨èŒèƒ½æè¿°
            manager_id INTEGER, -- éƒ¨é—¨ç»ç†çš„å‘˜å·¥IDï¼Œç”¨äºéƒ¨é—¨è´Ÿè´£äººåˆ†æ
            budget REAL NOT NULL, -- éƒ¨é—¨å¹´åº¦é¢„ç®—é‡‘é¢
            location TEXT -- éƒ¨é—¨åŠå…¬åœ°ç‚¹
        );""")
        
        # 2. åœ°åŒºè¡¨ - é”€å”®åŒºåŸŸå’Œç¨åŠ¡ä¿¡æ¯
        cursor.execute("""
        CREATE TABLE regions (
            id INTEGER PRIMARY KEY, 
            name TEXT NOT NULL UNIQUE, -- åœ°åŒºåç§°ï¼Œå¦‚ï¼šååŒ—ã€åä¸œã€åå—ã€æ¬§æ´²ç­‰
            country TEXT NOT NULL, -- æ‰€å±å›½å®¶
            timezone TEXT, -- æ—¶åŒºä¿¡æ¯
            tax_rate REAL -- è¯¥åœ°åŒºçš„ç¨ç‡ï¼Œç”¨äºç¨åŠ¡åˆ†æå’Œä»·æ ¼æ•æ„Ÿæ€§åˆ†æ
        );""")
        
        # 3. äº§å“è¡¨ - å•†å“ä¿¡æ¯å’Œæˆæœ¬æ•°æ®
        cursor.execute("""
        CREATE TABLE products (
            id INTEGER PRIMARY KEY, 
            name TEXT NOT NULL, -- äº§å“åç§°ï¼Œå¦‚ï¼šç¬”è®°æœ¬ç”µè„‘ã€æ™ºèƒ½æ‰‹æœºç­‰
            category TEXT NOT NULL, -- äº§å“ç±»åˆ«ï¼Œå¦‚ï¼šç”µè„‘ã€æ‰‹æœºã€å®¶ç”µã€æœè£…
            price REAL NOT NULL, -- é”€å”®ä»·æ ¼
            cost REAL NOT NULL, -- äº§å“æˆæœ¬ï¼Œç”¨äºåˆ©æ¶¦åˆ†æ
            supplier_id INTEGER, -- ä¾›åº”å•†IDï¼Œå…³è”ä¾›åº”å•†è¡¨
            launch_date TEXT, -- äº§å“ä¸Šå¸‚æ—¥æœŸ
            FOREIGN KEY (supplier_id) REFERENCES suppliers (id)
        );""")
        
        # 4. å‘˜å·¥è¡¨ - äººåŠ›èµ„æºå’Œç»„ç»‡å±‚çº§
        cursor.execute("""
        CREATE TABLE employees (
            id INTEGER PRIMARY KEY,
            name TEXT NOT NULL, -- å‘˜å·¥å§“å
            department_id INTEGER NOT NULL, -- æ‰€å±éƒ¨é—¨ID
            position TEXT, -- èŒä½åç§°ï¼Œå¦‚ï¼šé”€å”®ç»ç†ã€å¼€å‘å·¥ç¨‹å¸ˆ
            salary REAL, -- å‘˜å·¥è–ªèµ„ï¼Œç”¨äºæˆæœ¬å’Œç»©æ•ˆåˆ†æ
            hire_date TEXT, -- å…¥èŒæ—¥æœŸ
            manager_id INTEGER, -- ç›´å±ä¸Šçº§ç»ç†IDï¼Œç”¨äºå‘˜å·¥å±‚çº§å…³ç³»å’Œç®¡ç†æ•ˆèƒ½åˆ†æï¼Œä¹Ÿå¯ç§°ä¸ºreports_toæˆ–supervisor_id
            FOREIGN KEY (department_id) REFERENCES departments (id),
            FOREIGN KEY (manager_id) REFERENCES employees (id)
        );""")
        
        # 5. å®¢æˆ·è¡¨ - å®¢æˆ·ä¿¡æ¯å’Œåˆ†ç±»
        cursor.execute("""
        CREATE TABLE customers (
            id INTEGER PRIMARY KEY,
            name TEXT NOT NULL, -- å®¢æˆ·åç§°æˆ–å…¬å¸å
            email TEXT, -- è”ç³»é‚®ç®±
            region_id INTEGER NOT NULL, -- æ‰€å±é”€å”®åŒºåŸŸ
            customer_type TEXT, -- å®¢æˆ·ç±»å‹ï¼šä¸ªäººå®¢æˆ·ã€ä¼ä¸šå®¢æˆ·ã€ä»£ç†å•†ã€åˆ†é”€å•†ã€é›¶å”®å•†ç­‰ï¼Œç”¨äºå®¢æˆ·è¡Œä¸ºåˆ†æ
            registration_date TEXT, -- æ³¨å†Œæ—¥æœŸ
            credit_limit REAL, -- ä¿¡ç”¨é¢åº¦
            FOREIGN KEY (region_id) REFERENCES regions (id)
        );""")
        
        # 6. ä¾›åº”å•†è¡¨ - ä¾›åº”é“¾ç®¡ç†
        cursor.execute("""
        CREATE TABLE suppliers (
            id INTEGER PRIMARY KEY,
            name TEXT NOT NULL, -- ä¾›åº”å•†åç§°
            region_id INTEGER NOT NULL, -- ä¾›åº”å•†æ‰€åœ¨åœ°åŒº
            contact_email TEXT, -- è”ç³»é‚®ç®±
            phone TEXT, -- è”ç³»ç”µè¯
            quality_rating REAL, -- ä¾›åº”å•†è´¨é‡è¯„çº§ï¼ˆ1-5åˆ†ï¼‰ï¼Œç”¨äºä¾›åº”å•†ç»©æ•ˆåˆ†æ
            FOREIGN KEY (region_id) REFERENCES regions (id)
        );""")
        
        # 7. è®¢å•è¡¨ - é”€å”®è®¢å•ä¸»è¡¨
        cursor.execute("""
        CREATE TABLE orders (
            id INTEGER PRIMARY KEY,
            customer_id INTEGER NOT NULL, -- ä¸‹å•å®¢æˆ·ID
            employee_id INTEGER NOT NULL, -- è´Ÿè´£é”€å”®å‘˜å·¥IDï¼Œç”¨äºé”€å”®ç»©æ•ˆåˆ†æ
            order_date TEXT NOT NULL, -- è®¢å•æ—¥æœŸï¼Œç”¨äºæ—¶é—´åˆ†æå’Œå­£èŠ‚æ€§åˆ†æ
            total_amount REAL, -- è®¢å•æ€»é‡‘é¢
            status TEXT, -- è®¢å•çŠ¶æ€ï¼šå·²å®Œæˆã€å¤„ç†ä¸­ã€å·²å–æ¶ˆã€Promotionï¼ˆä¿ƒé”€æœŸé—´ï¼‰ç­‰
            shipping_region_id INTEGER, -- å‘è´§åœ°åŒºID
            FOREIGN KEY (customer_id) REFERENCES customers (id),
            FOREIGN KEY (employee_id) REFERENCES employees (id),
            FOREIGN KEY (shipping_region_id) REFERENCES regions (id)
        );""")
        
        # 8. é”€å”®æ˜ç»†è¡¨ - è®¢å•å•†å“æ˜ç»†ï¼Œç›¸å½“äºè®¢å•æ˜ç»†è¡¨(order_items/order_details)
        cursor.execute("""
        CREATE TABLE sales (
            id INTEGER PRIMARY KEY, 
            order_id INTEGER NOT NULL, -- å…³è”è®¢å•ID
            product_id INTEGER NOT NULL, -- é”€å”®äº§å“ID
            quantity INTEGER NOT NULL, -- é”€å”®æ•°é‡
            unit_price REAL NOT NULL, -- å•†å“å•ä»·
            discount REAL DEFAULT 0, -- æŠ˜æ‰£ç‡ï¼ˆ0-1ä¹‹é—´ï¼‰ï¼Œç”¨äºä¿ƒé”€æ•ˆæœåˆ†æ
            FOREIGN KEY (order_id) REFERENCES orders (id),
            FOREIGN KEY (product_id) REFERENCES products (id)
        );""")
        
        # 9. åº“å­˜è¡¨ - åº“å­˜ç®¡ç†å’Œé£é™©åˆ†æ
        cursor.execute("""
        CREATE TABLE inventory (
            id INTEGER PRIMARY KEY,
            product_id INTEGER NOT NULL, -- äº§å“ID
            region_id INTEGER NOT NULL, -- åº“å­˜æ‰€åœ¨åœ°åŒº
            quantity_in_stock INTEGER, -- å½“å‰åº“å­˜æ•°é‡
            reorder_level INTEGER, -- å†è®¢è´­æ°´å¹³ï¼Œç”¨äºåº“å­˜ç§¯å‹é£é™©åˆ†æå’Œè¡¥è´§é¢„è­¦
            last_updated TEXT, -- æœ€åæ›´æ–°æ—¶é—´
            FOREIGN KEY (product_id) REFERENCES products (id),
            FOREIGN KEY (region_id) REFERENCES regions (id)
        );""")
        
        # 10. ä¿ƒé”€æ´»åŠ¨è¡¨ - è¥é”€æ´»åŠ¨ç®¡ç†
        cursor.execute("""
        CREATE TABLE promotions (
            id INTEGER PRIMARY KEY,
            name TEXT NOT NULL, -- ä¿ƒé”€æ´»åŠ¨åç§°
            product_id INTEGER, -- ä¿ƒé”€äº§å“IDï¼ŒNULLè¡¨ç¤ºå…¨å“ç±»ä¿ƒé”€
            region_id INTEGER, -- ä¿ƒé”€åœ°åŒºIDï¼ŒNULLè¡¨ç¤ºå…¨åœ°åŒº
            discount_rate REAL, -- ä¿ƒé”€æŠ˜æ‰£ç‡ï¼Œç”¨äºä¿ƒé”€æ•ˆæœåˆ†æå’Œç´§æ€¥è¡¥è´§é¢„è­¦
            start_date TEXT, -- ä¿ƒé”€å¼€å§‹æ—¥æœŸ
            end_date TEXT, -- ä¿ƒé”€ç»“æŸæ—¥æœŸï¼Œç”¨äºè¯†åˆ«å³å°†è¿‡æœŸçš„ä¿ƒé”€æ´»åŠ¨
            FOREIGN KEY (product_id) REFERENCES products (id),
            FOREIGN KEY (region_id) REFERENCES regions (id)
        );""")
        
        logger.info("10 tables created for complex enterprise scenario.")
    
    def _insert_sample_data(self, cursor: sqlite3.Cursor):
        """Inserts comprehensive sample data for complex multi-table queries."""
        # 1. éƒ¨é—¨æ•°æ®
        cursor.executemany("INSERT INTO departments VALUES (?, ?, ?, ?, ?, ?)", [
            (1, 'ç”µå­äº§å“', 'æ¶ˆè´¹ç”µå­äº§å“é”€å”®éƒ¨é—¨', 1, 2000000, 'åŒ—äº¬'),
            (2, 'å®¶å±…ç”¨å“', 'å®¶å±…ç”Ÿæ´»ç”¨å“éƒ¨é—¨', 3, 800000, 'ä¸Šæµ·'),
            (3, 'æœè£…', 'æ—¶å°šæœè£…éƒ¨é—¨', 4, 1200000, 'å¹¿å·'),
            (4, 'å¸‚åœºè¥é”€', 'å“ç‰Œæ¨å¹¿å’Œå¸‚åœºåˆ†æ', 5, 600000, 'æ·±åœ³'),
            (5, 'äººåŠ›èµ„æº', 'å‘˜å·¥ç®¡ç†å’Œæ‹›è˜', 6, 400000, 'æ­å·')
        ])
        
        # 2. åœ°åŒºæ•°æ®
        cursor.executemany("INSERT INTO regions VALUES (?, ?, ?, ?, ?)", [
            (1, 'ååŒ—', 'ä¸­å›½', 'GMT+8', 0.13),
            (2, 'åä¸œ', 'ä¸­å›½', 'GMT+8', 0.13),
            (3, 'åå—', 'ä¸­å›½', 'GMT+8', 0.13),
            (4, 'åŒ—ç¾', 'ç¾å›½', 'GMT-5', 0.08),
            (5, 'æ¬§æ´²', 'å¾·å›½', 'GMT+1', 0.19),
            (6, 'ä¸œå—äºš', 'æ–°åŠ å¡', 'GMT+8', 0.07)
        ])
        
        # 3. ä¾›åº”å•†æ•°æ®
        cursor.executemany("INSERT INTO suppliers VALUES (?, ?, ?, ?, ?, ?)", [
            (1, 'æ·±åœ³ç§‘æŠ€æœ‰é™å…¬å¸', 3, 'tech@shenzhen.com', '0755-1234567', 4.5),
            (2, 'æ±Ÿè‹åˆ¶é€ é›†å›¢', 2, 'sales@jiangsu.com', '025-7654321', 4.2),
            (3, 'å¹¿ä¸œçººç»‡å…¬å¸', 3, 'textile@guangdong.com', '020-9876543', 4.0),
            (4, 'åŒ—äº¬åˆ›æ–°ç§‘æŠ€', 1, 'innovation@beijing.com', '010-5555555', 4.8)
        ])
        
        # 4. äº§å“æ•°æ®
        cursor.executemany("INSERT INTO products VALUES (?, ?, ?, ?, ?, ?, ?)", [
            (1, 'ç¬”è®°æœ¬ç”µè„‘', 'ç”µè„‘', 1200, 800, 1, '2023-01-15'),
            (2, 'æ™ºèƒ½æ‰‹æœº', 'æ‰‹æœº', 800, 500, 1, '2023-03-20'),
            (3, 'å¹³æ¿ç”µè„‘', 'ç”µè„‘', 600, 400, 1, '2023-05-10'),
            (4, 'å’–å•¡æœº', 'å®¶ç”µ', 150, 100, 2, '2023-02-01'),
            (5, 'ç©ºæ°”å‡€åŒ–å™¨', 'å®¶ç”µ', 300, 200, 2, '2023-04-15'),
            (6, 'Tæ¤', 'ä¸Šè¡£', 25, 15, 3, '2023-03-01'),
            (7, 'ç‰›ä»”è£¤', 'è£¤å­', 80, 50, 3, '2023-06-01'),
            (8, 'æ— çº¿è€³æœº', 'æ•°ç ', 200, 120, 4, '2023-07-01')
        ])
        
        # 5. å‘˜å·¥æ•°æ®
        cursor.executemany("INSERT INTO employees VALUES (?, ?, ?, ?, ?, ?, ?)", [
            (1, 'å¼ ä¸‰', 1, 'é”€å”®ç»ç†', 120000, '2020-01-15', None),
            (2, 'æå››', 1, 'é”€å”®ä»£è¡¨', 80000, '2021-03-22', 1),
            (3, 'ç‹äº”', 2, 'äº§å“ç»ç†', 95000, '2019-07-30', None),
            (4, 'èµµå…­', 3, 'è®¾è®¡å¸ˆ', 75000, '2022-02-14', None),
            (5, 'é’±ä¸ƒ', 4, 'å¸‚åœºä¸“å‘˜', 65000, '2021-11-08', None),
            (6, 'å­™å…«', 5, 'HRä¸“å‘˜', 55000, '2023-01-10', None),
            (7, 'å‘¨ä¹', 1, 'é”€å”®ä»£è¡¨', 85000, '2022-09-05', 1),
            (8, 'å´å', 2, 'é‡‡è´­å‘˜', 70000, '2020-12-01', 3)
        ])
        
        # 6. å®¢æˆ·æ•°æ®
        cursor.executemany("INSERT INTO customers VALUES (?, ?, ?, ?, ?, ?, ?)", [
            (1, 'åŒ—äº¬ç§‘æŠ€å…¬å¸', 'beijing@tech.com', 1, 'ä¼ä¸šå®¢æˆ·', '2022-01-01', 500000),
            (2, 'ä¸Šæµ·è´¸æ˜“å…¬å¸', 'shanghai@trade.com', 2, 'ä¼ä¸šå®¢æˆ·', '2022-03-15', 300000),
            (3, 'å¹¿å·é›¶å”®å•†', 'guangzhou@retail.com', 3, 'é›¶å”®å•†', '2022-05-20', 200000),
            (4, 'ä¸ªäººå®¢æˆ·å¼ å…ˆç”Ÿ', 'zhang@personal.com', 1, 'ä¸ªäººå®¢æˆ·', '2023-01-10', 50000),
            (5, 'ä¸ªäººå®¢æˆ·æå¥³å£«', 'li@personal.com', 2, 'ä¸ªäººå®¢æˆ·', '2023-02-15', 30000),
            (6, 'æµ·å¤–ä»£ç†å•†A', 'agent@overseas.com', 4, 'ä»£ç†å•†', '2022-08-01', 1000000),
            (7, 'æ¬§æ´²åˆ†é”€å•†', 'eu@distributor.com', 5, 'åˆ†é”€å•†', '2022-10-01', 800000)
        ])
        
        # 7. è®¢å•æ•°æ®
        cursor.executemany("INSERT INTO orders VALUES (?, ?, ?, ?, ?, ?, ?)", [
            (1, 1, 1, '2024-01-15', 15000, 'å·²å®Œæˆ', 1),
            (2, 2, 2, '2024-02-20', 8000, 'å·²å®Œæˆ', 2),
            (3, 3, 1, '2024-03-10', 2500, 'å·²å®Œæˆ', 3),
            (4, 4, 7, '2024-04-05', 1200, 'å·²å®Œæˆ', 1),
            (5, 5, 2, '2024-05-18', 300, 'å·²å®Œæˆ', 2),
            (6, 6, 1, '2024-06-22', 50000, 'å¤„ç†ä¸­', 4),
            (7, 7, 7, '2024-07-01', 25000, 'å·²å‘è´§', 5),
            (8, 1, 1, '2024-07-15', 18000, 'å·²å®Œæˆ', 1),
            (9, 3, 2, '2024-08-01', 4000, 'å¤„ç†ä¸­', 3)
        ])
        
        # 8. é”€å”®æ˜ç»†æ•°æ®
        cursor.executemany("INSERT INTO sales VALUES (?, ?, ?, ?, ?, ?)", [
            (1, 1, 1, 10, 1200, 0.1),    # åŒ—äº¬ç§‘æŠ€å…¬å¸ä¹°ç¬”è®°æœ¬
            (2, 1, 2, 5, 800, 0.05),     # åŒ—äº¬ç§‘æŠ€å…¬å¸ä¹°æ‰‹æœº
            (3, 2, 2, 10, 800, 0.0),     # ä¸Šæµ·è´¸æ˜“å…¬å¸ä¹°æ‰‹æœº
            (4, 3, 4, 10, 150, 0.0),     # å¹¿å·é›¶å”®å•†ä¹°å’–å•¡æœº
            (5, 3, 6, 50, 25, 0.0),      # å¹¿å·é›¶å”®å•†ä¹°Tæ¤
            (6, 4, 1, 1, 1200, 0.0),     # å¼ å…ˆç”Ÿä¹°ç¬”è®°æœ¬
            (7, 5, 5, 1, 300, 0.0),      # æå¥³å£«ä¹°ç©ºæ°”å‡€åŒ–å™¨
            (8, 6, 1, 30, 1200, 0.15),   # æµ·å¤–ä»£ç†å•†ä¹°ç¬”è®°æœ¬
            (9, 6, 3, 20, 600, 0.1),     # æµ·å¤–ä»£ç†å•†ä¹°å¹³æ¿
            (10, 7, 2, 25, 800, 0.12),   # æ¬§æ´²åˆ†é”€å•†ä¹°æ‰‹æœº
            (11, 7, 8, 10, 200, 0.08),   # æ¬§æ´²åˆ†é”€å•†ä¹°è€³æœº
            (12, 8, 1, 15, 1200, 0.08),  # åŒ—äº¬ç§‘æŠ€å…¬å¸å†æ¬¡è´­ä¹°
            (13, 9, 7, 40, 80, 0.0)      # å¹¿å·é›¶å”®å•†ä¹°ç‰›ä»”è£¤
        ])
        
        # 9. åº“å­˜æ•°æ®
        cursor.executemany("INSERT INTO inventory VALUES (?, ?, ?, ?, ?, ?)", [
            (1, 1, 1, 100, 20, '2024-08-01'),  # ååŒ—ç¬”è®°æœ¬åº“å­˜
            (2, 1, 2, 80, 15, '2024-08-01'),   # åä¸œç¬”è®°æœ¬åº“å­˜
            (3, 2, 1, 200, 50, '2024-08-01'),  # ååŒ—æ‰‹æœºåº“å­˜
            (4, 2, 3, 150, 30, '2024-08-01'),  # åå—æ‰‹æœºåº“å­˜
            (5, 4, 2, 50, 10, '2024-08-01'),   # åä¸œå’–å•¡æœºåº“å­˜
            (6, 6, 3, 500, 100, '2024-08-01'), # åå—Tæ¤åº“å­˜
            (7, 8, 5, 80, 20, '2024-08-01')    # æ¬§æ´²è€³æœºåº“å­˜
        ])
        
        # 10. ä¿ƒé”€æ´»åŠ¨æ•°æ®
        cursor.executemany("INSERT INTO promotions VALUES (?, ?, ?, ?, ?, ?, ?)", [
            (1, 'æ˜¥å­£ç”µè„‘ä¿ƒé”€', 1, 1, 0.15, '2024-03-01', '2024-03-31'),
            (2, 'å¤å­£æ‰‹æœºä¼˜æƒ ', 2, 2, 0.1, '2024-06-01', '2024-06-30'),
            (3, 'å…¨çƒTæ¤èŠ‚', 6, None, 0.2, '2024-07-01', '2024-07-15'),
            (4, 'å®¶ç”µæ¸…ä»“', None, 2, 0.25, '2024-08-01', '2024-08-31')
        ])
        
        logger.info("Complex sample data inserted for 10-table scenario.")
    
    def get_all_schemas(self) -> List[TableSchema]:
        """Retrieves DDL and descriptions for all tables in the database."""
        conn = sqlite3.connect(self.db_path)
        cursor = conn.cursor()
        cursor.execute("SELECT name, sql FROM sqlite_master WHERE type='table' AND name NOT LIKE 'sqlite_%'")
        tables = cursor.fetchall()
        conn.close()
        
        # Enhanced descriptions for complex scenario
        descriptions = {
            'departments': 'éƒ¨é—¨ä¿¡æ¯è¡¨ï¼ŒåŒ…å«éƒ¨é—¨é¢„ç®—ã€ä½ç½®ã€ç»ç†ç­‰è¯¦ç»†ä¿¡æ¯ã€‚',
            'regions': 'åœ°åŒºè¡¨ï¼Œå­˜å‚¨å…¨çƒå„åœ°åŒºçš„æ—¶åŒºã€ç¨ç‡ç­‰ä¿¡æ¯ã€‚',
            'products': 'äº§å“ç›®å½•è¡¨ï¼ŒåŒ…å«ä»·æ ¼ã€æˆæœ¬ã€ä¾›åº”å•†ã€ä¸Šå¸‚æ—¶é—´ç­‰å®Œæ•´äº§å“ä¿¡æ¯ã€‚',
            'employees': 'å‘˜å·¥æ¡£æ¡ˆè¡¨ï¼Œè®°å½•èŒä½ã€è–ªèµ„ã€å…¥èŒæ—¶é—´ã€ä¸Šçº§å…³ç³»ç­‰ã€‚',
            'customers': 'å®¢æˆ·ä¿¡æ¯è¡¨ï¼ŒåŒ…å«å®¢æˆ·ç±»å‹ã€ä¿¡ç”¨é¢åº¦ã€æ³¨å†Œæ—¶é—´ç­‰ã€‚',
            'suppliers': 'ä¾›åº”å•†ç®¡ç†è¡¨ï¼Œè®°å½•è”ç³»æ–¹å¼ã€è´¨é‡è¯„çº§ç­‰ä¾›åº”å•†ä¿¡æ¯ã€‚',
            'orders': 'è®¢å•ä¸»è¡¨ï¼Œè®°å½•å®¢æˆ·è®¢å•çš„åŸºæœ¬ä¿¡æ¯å’ŒçŠ¶æ€ã€‚',
            'sales': 'é”€å”®æ˜ç»†è¡¨ï¼Œè®°å½•æ¯ä¸ªè®¢å•ä¸­å…·ä½“äº§å“çš„é”€å”®æƒ…å†µã€‚',
            'inventory': 'åº“å­˜ç®¡ç†è¡¨ï¼Œè·Ÿè¸ªå„åœ°åŒºäº§å“åº“å­˜é‡å’Œè¡¥è´§æé†’ã€‚',
            'promotions': 'ä¿ƒé”€æ´»åŠ¨è¡¨ï¼Œç®¡ç†äº§å“å’Œåœ°åŒºçš„ä¼˜æƒ æ´»åŠ¨ä¿¡æ¯ã€‚'
        }

        return [TableSchema(name=t[0], ddl=t[1], description=descriptions.get(t[0], '')) for t in tables]

    def execute_sql(self, sql: str) -> QueryResult:
        """Executes a given SQL query and returns the result."""
        logger.info(f"Executing SQL: {sql.strip()}")
        try:
            with sqlite3.connect(self.db_path) as conn:
                conn.row_factory = sqlite3.Row
                cursor = conn.cursor()
                cursor.execute(sql)
                rows = cursor.fetchall()
                data = [dict(row) for row in rows]
                logger.info(f"SQL executed successfully, returned {len(data)} rows.")
                return QueryResult(success=True, data=data, sql=sql)
        except Exception as e:
            logger.error(f"SQL execution failed: {e}", exc_info=True)
            return QueryResult(success=False, data=[], error=str(e), sql=sql)

class VectorStore:
    """Handles embedding creation and retrieval of relevant schemas using DashScope."""
    def __init__(self, model_name: str = "text-embedding-v4"):
        try:
            if OpenAI is None:
                raise ImportError("OpenAI package not installed. Please run 'pip install openai'.")
            self.model_name = model_name
            self.client = OpenAI(
                api_key=os.getenv("DASHSCOPE_API_KEY"),
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
            logger.info(f"VectorStore initialized with DashScope model: {model_name}")
        except Exception as e:
            logger.error(f"Failed to initialize DashScope embedding client: {e}", exc_info=True)
            raise
        self.schemas: List[TableSchema] = []
        self.schema_embeddings: Optional[np.ndarray] = None

    def get_embeddings(self, texts: List[str]) -> np.ndarray:
        """Get embeddings using DashScope API"""
        try:
            # æ‰¹é‡å¤„ç†æ–‡æœ¬
            all_embeddings = []
            for text in texts:
                response = self.client.embeddings.create(
                    model=self.model_name,
                    input=text,
                    dimensions=1024,
                    encoding_format="float"
                )
                all_embeddings.append(response.data[0].embedding)
            return np.array(all_embeddings)
        except Exception as e:
            logger.error(f"Failed to get embeddings: {e}")
            raise

    def build_embeddings(self, schemas: List[TableSchema]):
        """Creates and stores vector embeddings for the given schemas."""
        self.schemas = schemas
        if not self.schemas:
            logger.warning("No schemas provided to build embeddings.")
            return
            
        # Create text descriptions for embedding
        descriptions = []
        for schema in self.schemas:
            # ä½¿ç”¨è¡¨å+æè¿°+DDLçš„ç»„åˆä½œä¸ºembeddingè¾“å…¥
            text = f"Table: {schema.name}\nDescription: {schema.description}\nDDL: {schema.ddl}"
            descriptions.append(text)
        
        logger.info(f"Creating embeddings for {len(descriptions)} schemas...")
        self.schema_embeddings = self.get_embeddings(descriptions)
        logger.info(f"Built embeddings for {len(self.schemas)} schemas.")

    def _extract_columns_from_ddl(self, ddl: str) -> str:
        """Extract column names from DDL for better context."""
        try:
            lines = ddl.strip().split('\n')
            column_lines = lines[1:-1] # Exclude CREATE TABLE and closing parenthesis
            return ", ".join([line.strip().split()[0] for line in column_lines])
        except Exception:
            return ""
    
    def retrieve_relevant_schemas(self, question: str, top_k: int = 5) -> List[TableSchema]:
        """Finds the most relevant schemas for a question using cosine similarity."""
        if self.schema_embeddings is None or not self.schemas:
            logger.warning("Embeddings not built. Cannot retrieve schemas.")
            return []
        
        question_embedding = self.get_embeddings([question])
        similarities = cosine_similarity(question_embedding, self.schema_embeddings)[0]
        
        # Get top-k indices, ensuring we don't exceed the number of available schemas
        k = min(top_k, len(self.schemas))
        top_indices = np.argsort(similarities)[-k:][::-1]
        
        relevant_schemas = [self.schemas[i] for i in top_indices]
        logger.info(f"Retrieved {len(relevant_schemas)} relevant schemas for the question.")
        for i in top_indices:
            logger.info(f"  - {self.schemas[i].name} (Similarity: {similarities[i]:.4f})")
            
        return relevant_schemas
    
    def multi_path_retrieve_schemas(self, question: str, llm_provider, top_k_per_path: int = 3) -> List[TableSchema]:
        """ä½¿ç”¨LLMåˆ†ææŸ¥è¯¢ç»´åº¦ï¼Œç„¶åå¤šè·¯å¬å›DDLè¡¨ç»“æ„"""
        
        # ç¬¬ä¸€æ­¥ï¼šè®©LLMåˆ†æéœ€è¦å“ªäº›æ•°æ®ç»´åº¦
        analysis_prompt = f"""
åˆ†æè¿™ä¸ªä¸šåŠ¡æŸ¥è¯¢éœ€è¦å“ªäº›æ•°æ®ç»´åº¦ï¼Œè¾“å‡º3-5ä¸ªå…·ä½“çš„æŸ¥è¯¢æ–¹å‘ã€‚
æ¯ä¸ªæ–¹å‘è¦ä½¿ç”¨æœ€ç›´æ¥ã€ç®€æ´çš„å…³é”®è¯ï¼Œä¾¿äºåŒ¹é…æ•°æ®è¡¨åç§°ã€‚

æŸ¥è¯¢: {question}

è¾“å‡ºæ ¼å¼ï¼Œæ¯è¡Œä¸€ä¸ªç»´åº¦ï¼š
employees
sales
products
customers
orders
"""
        
        logger.info("LLM analyzing query dimensions...")
        dimensions_text = llm_provider._call_llm(analysis_prompt, "qwen-plus")
        
        # è§£æåˆ†æç»“æœ
        dimensions = [dim.strip() for dim in dimensions_text.split('\n') if dim.strip()]
        logger.info(f"Identified {len(dimensions)} query dimensions: {dimensions}")
        
        # ç¬¬äºŒæ­¥ï¼šä¸ºæ¯ä¸ªç»´åº¦è¿›è¡Œå‘é‡æ£€ç´¢
        all_retrieved_schemas = []
        seen_table_names = set()
        
        for dimension in dimensions:
            logger.info(f"Retrieving dimension: {dimension}")
            
            dimension_embedding = self.get_embeddings([dimension])
            similarities = cosine_similarity(dimension_embedding, self.schema_embeddings)[0]
            
            # ä¸ºæ¯ä¸ªç»´åº¦æ£€ç´¢top_k_per_pathä¸ªè¡¨
            k = min(top_k_per_path, len(self.schemas))
            top_indices = np.argsort(similarities)[-k:][::-1]
            
            for i in top_indices:
                schema = self.schemas[i]
                if schema.name not in seen_table_names:
                    all_retrieved_schemas.append(schema)
                    seen_table_names.add(schema.name)
                    logger.info(f"  Retrieved {schema.name} (Similarity: {similarities[i]:.4f})")
        
        logger.info(f"Multi-path retrieval completed, retrieved {len(all_retrieved_schemas)} relevant tables")
        return all_retrieved_schemas
    
class LLMProvider:
    """A wrapper for LLM API calls using OpenAI-compatible interface."""
    def __init__(self, llm_config: Dict[str, Any]):
        self.provider = llm_config.get("provider")
        self.models = llm_config.get("models", {})
        api_key = os.environ.get(llm_config.get("api_key_env", ""))
        
        if not api_key:
            raise ValueError(f"API key not found. Please set the {llm_config.get('api_key_env')} environment variable.")

        if OpenAI is None:
            raise ImportError("OpenAI SDK not installed. Please run 'pip install openai'.")
        
        # ä½¿ç”¨OpenAIå…¼å®¹æ¥å£è°ƒç”¨DashScope
        if self.provider == "dashscope":
            self.client = OpenAI(
                api_key=api_key,
                base_url="https://dashscope.aliyuncs.com/compatible-mode/v1"
            )
        elif self.provider == "openai":
            self.client = OpenAI(api_key=api_key)
        else:
            raise ValueError(f"Unsupported LLM provider: {self.provider}")
        
        logger.info(f"LLMProvider initialized for '{self.provider}'.")

    def _call_llm(self, prompt: str, model: str) -> str:
        """Internal method to make the actual API call."""
        logger.info(f"Calling LLM ({self.provider}, model: {model})...")
        try:
            response = self.client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                temperature=0.0
            )
            return response.choices[0].message.content or ""
        except Exception as e:
            logger.error(f"LLM API call failed: {e}", exc_info=True)
            return f"Error: LLM call failed. {e}"

    def generate_sql(self, prompt: str) -> str:
        """Generates SQL from a prompt."""
        model = self.models.get("sql_generation", "qwen-plus")
        sql = self._call_llm(prompt, model)
        # Clean up potential markdown formatting
        return sql.replace("```sql", "").replace("```", "").strip()
    
    def generate_answer(self, prompt: str) -> str:
        """Generates a natural language answer from a prompt."""
        model = self.models.get("answer_generation", "qwen-plus")
        return self._call_llm(prompt, model).strip()

# --- Main Pipeline Orchestrator ----------------------------------------------

class NL2SQLPipeline:
    """Orchestrates the Text-to-SQL process using modular components."""
    def __init__(self, config: Dict[str, Any]):
        logger.info("Initializing NL2SQL Pipeline...")
        self.db_manager = DBManager(config['database'])
        
        # Initialize vector store
        self.vector_store = VectorStore(config['embedding_model'])
        logger.info(f"VectorStore initialized with embedding model: {config['embedding_model']}")
        
        # Initialize LLM provider  
        self.llm_provider = LLMProvider(llm_config=config['llm'])
        logger.info(f"LLMProvider initialized for '{config['llm']['provider']}'.")
        
        # Create embeddings for all schemas
        all_schemas = self.db_manager.get_all_schemas()
        logger.info(f"Creating embeddings for {len(all_schemas)} schemas...")
        self.vector_store.build_embeddings(all_schemas)
        logger.info(f"Built embeddings for {len(all_schemas)} schemas.")
        
        # Load prompt templates
        self.sql_prompt_template = config['prompts']['sql_generation']
        self.answer_prompt_template = config['prompts']['answer_generation']
        logger.info("NL2SQL Pipeline initialized successfully.")

    def ask(self, question: str) -> Dict[str, Any]:
        """
        Executes the full Text-to-SQL pipeline for a given question.
        """
        import time
        start_time = time.time()
        
        logger.info("=" * 80)
        logger.info(f"Processing question: {question}")
        logger.info("=" * 80)

        # 1. Retrieve relevant schemas using multi-path approach
        logger.info("Step 1: Starting multi-path vector retrieval...")
        retrieval_start = time.time()
        relevant_schemas = self.vector_store.multi_path_retrieve_schemas(question, self.llm_provider, top_k_per_path=4)
        retrieval_time = time.time() - retrieval_start
        
        logger.info(f"Vector retrieval completed in {retrieval_time:.2f}s")
        logger.info(f"Retrieved {len(relevant_schemas)} relevant tables: {[s.name for s in relevant_schemas]}")
        
        schema_context = "\n\n".join([f"--- Table: {s.name} ---\n{s.ddl}" for s in relevant_schemas])
        logger.info(f"Schema context built, length: {len(schema_context)} characters")

        # 2. Generate SQL
        logger.info("Step 2: Starting SQL generation...")
        sql_start = time.time()
        sql_prompt = self.sql_prompt_template.format(schema_context=schema_context, question=question)
        logger.info(f"SQL prompt length: {len(sql_prompt)} characters")
        
        sql_query = self.llm_provider.generate_sql(sql_prompt)
        sql_time = time.time() - sql_start
        
        logger.info(f"SQL generation completed in {sql_time:.2f}s")
        
        # Check if LLM rejected the query due to insufficient schema
        if sql_query.strip().startswith("SCHEMA_INSUFFICIENT:"):
            logger.warning("LLM rejected SQL generation due to insufficient DDL")
            logger.warning(f"Rejection reason: {sql_query.strip()}")
            
            total_time = time.time() - start_time
            return {
                'question': question,
                'relevant_schemas': [s.name for s in relevant_schemas],
                'sql_query': None,
                'data': [],
                'answer': f"Current database structure cannot satisfy the query requirements. {sql_query.replace('SCHEMA_INSUFFICIENT:', '').strip()}",
                'query_success': False,
                'schema_insufficient': True,
                'performance': {
                    'retrieval_time': retrieval_time,
                    'sql_generation_time': sql_time,
                    'execution_time': 0,
                    'answer_generation_time': 0,
                    'total_time': total_time
                }
            }
        
        logger.info(f"Generated SQL: {sql_query}")

        # 3. Execute SQL
        logger.info("Step 3: Starting database query execution...")
        exec_start = time.time()
        query_result = self.db_manager.execute_sql(sql_query)
        exec_time = time.time() - exec_start
        
        if query_result.success:
            logger.info(f"Query executed successfully in {exec_time:.2f}s")
            logger.info(f"Returned {len(query_result.data)} records")
            if query_result.data:
                logger.info(f"Sample data available: {len(query_result.data)} records")
        else:
            logger.error(f"Query execution failed in {exec_time:.2f}s")
            logger.error(f"Error: {query_result.error}")

        # 4. Generate Answer (if SQL was successful)
        logger.info("Step 4: Starting natural language answer generation...")
        answer_start = time.time()
        answer = ""
        
        if query_result.success:
            if not query_result.data:
                answer = "I found relevant information, but no data matched your specific conditions."
                logger.info("Query successful but no data, returning standard prompt")
            else:
                # ç”Ÿæˆæ•°æ®æ‘˜è¦è€Œä¸æ˜¯å®Œæ•´æ•°æ®ï¼Œé¿å…æ•°æ®æ³„éœ²
                data_summary = self._create_data_summary(query_result.data)
                logger.info(f"Preparing answer generation, data summary length: {len(data_summary)} characters")
                
                answer_prompt = self.answer_prompt_template.format(
                    question=question,
                    sql_query=sql_query,
                    data_summary=data_summary
                )
                logger.info(f"Answer generation prompt length: {len(answer_prompt)} characters")
                
                answer = self.llm_provider.generate_answer(answer_prompt)
                logger.info(f"Answer length: {len(answer)} characters")
        else:
            answer = f"Sorry, an error occurred while answering your question. Database reported: {query_result.error}"
            logger.info("Due to query failure, returning error message")
        
        answer_time = time.time() - answer_start
        total_time = time.time() - start_time
        
        logger.info(f"Answer generation completed in {answer_time:.2f}s")
        logger.info("Performance statistics:")
        logger.info(f"   Retrieval: {retrieval_time:.2f}s")
        logger.info(f"   SQL Generation: {sql_time:.2f}s") 
        logger.info(f"   Query Execution: {exec_time:.2f}s")
        logger.info(f"   Answer Generation: {answer_time:.2f}s")
        logger.info(f"   Total time: {total_time:.2f}s")
        logger.info("=" * 80)
        logger.info(f"Question processing completed: {question[:50]}...")
        logger.info("=" * 80)
        
        return {
            "question": question,
            "relevant_schemas": [s.name for s in relevant_schemas],
            "sql_query": query_result.sql,
            "query_success": query_result.success,
            "query_error": query_result.error,
            "data": query_result.data,
            "answer": answer,
            "performance": {
                "retrieval_time": retrieval_time,
                "sql_generation_time": sql_time,
                "execution_time": exec_time,
                "answer_generation_time": answer_time,
                "total_time": total_time
            }
        }

    def _create_data_summary(self, data: List[Dict[str, Any]]) -> str:
        """åˆ›å»ºæ•°æ®æ‘˜è¦ï¼Œé¿å…æ³„éœ²æ•æ„Ÿä¿¡æ¯ï¼Œåªæä¾›ç»“æ„åŒ–ç»Ÿè®¡ä¿¡æ¯"""
        if not data:
            return "No data available."

        # æå–æ‰€æœ‰åˆ—åå’Œæ•°æ®ç±»å‹ä¿¡æ¯
        all_columns = set()
        column_types = {}
        
        for record in data:
            all_columns.update(record.keys())
            for key, value in record.items():
                if key not in column_types:
                    column_types[key] = type(value).__name__

        # ç”Ÿæˆå®‰å…¨çš„æ•°æ®æ‘˜è¦ï¼ˆä¸åŒ…å«å®é™…æ•°æ®å€¼ï¼‰
        summary_data = {
            "total_records": len(data),
            "columns_info": {
                col: column_types.get(col, "unknown") 
                for col in sorted(list(all_columns))
            },
            "data_structure": "Multi-table query results with business metrics",
            "privacy_note": "Actual data values omitted for security"
        }
        
        return json.dumps(summary_data, indent=2, ensure_ascii=False)

# --- Demo Execution ---------------------------------------------------------
def run_demo():
    """Sets up the pipeline and runs a demo with complex BI questions."""
    print("=" * 60)
    print("ğŸ¤– ä¼ä¸šçº§ä¸­æ–‡NL2SQLè‡ªåŠ¨æ¼”ç¤º")
    print("=" * 60)

    try:
        # Check for API key before initializing
        api_key_env = CONFIG['llm']['api_key_env']
        if not os.environ.get(api_key_env):
            print(f"\nâŒ é”™è¯¯ï¼šç¯å¢ƒå˜é‡ '{api_key_env}' æœªè®¾ç½®ã€‚")
            print("è¯·è®¾ç½®æ‚¨çš„APIå¯†é’¥ä»¥ç»§ç»­ã€‚")
            return
    
        pipeline = NL2SQLPipeline(CONFIG)
        
        # å¤æ‚çš„å¤šè¡¨è”æŸ¥é—®é¢˜é›†åˆ
        demo_questions = [
            {
                "question": "åˆ†æä¸åŒåœ°åŒºç¨ç‡å¯¹è®¢å•é‡‘é¢å’Œå®¢æˆ·è´­ä¹°å†³ç­–çš„å½±å“ï¼Œæ‰¾å‡ºæœ€å…·ä»·æ ¼æ•æ„Ÿæ€§çš„å®¢æˆ·ç¾¤ä½“ã€‚",
                "description": "ğŸŒ ç¨ç‡å½±å“åˆ†æ - å¤šè¡¨è”æŸ¥ + ä»·æ ¼æ•æ„Ÿæ€§åˆ†æ"
            },
            {
                "question": "åˆ†æå„åœ°åŒºçš„é”€å”®ä¸šç»©å’Œäº§å“è¡¨ç°ï¼šç»Ÿè®¡æ¯ä¸ªåœ°åŒºçš„æ€»é”€å”®é¢ã€ç•…é”€äº§å“ç±»åˆ«ã€å®¢æˆ·ç±»å‹åˆ†å¸ƒï¼Œä»¥åŠå¹³å‡æŠ˜æ‰£æ°´å¹³ï¼Œè¯†åˆ«å‡ºé”€å”®æ½œåŠ›æœ€å¤§çš„åœ°åŒºå’Œäº§å“ç»„åˆã€‚",
                "description": "ğŸ“ˆ åœ°åŒºé”€å”®æ½œåŠ›åˆ†æ - 8è¡¨è”æŸ¥ + äº§å“è¡¨ç° + å®¢æˆ·ç»†åˆ†"
            }
        ]
        
        print(f"\nEnterprise complex scenario demo started - showcasing the most challenging multi-table query analysis")
        print("Database scale: 10 core business tables with complex relationships and business logic")
        print("Challenge difficulty: Tax impact analysis - requires intelligent identification of 8+ related tables for price sensitivity analysis")
        print("=" * 90)
        
        # ç»Ÿè®¡ä¿¡æ¯
        import time
        total_start_time = time.time()
        demo_stats = {
            "total_questions": len(demo_questions),
            "successful_queries": 0,
            "failed_queries": 0,
            "total_tables_used": set(),
            "total_execution_time": 0,
            "performance_breakdown": [],
            "schema_insufficient_queries": 0
        }
        
        logger.info("Starting enterprise NL2SQL demo")
        logger.info(f"Demo configuration: {len(demo_questions)} complex questions, 10 tables")
        logger.info("=" * 80)
        
        for i, demo in enumerate(demo_questions, 1):
            question = demo["question"]
            description = demo["description"]
            
            print(f"\n[Challenge {i}/{len(demo_questions)}] {description}")
            print(f"Question: {question}")
            print("Processing...")
            
            logger.info("=" * 30)
            logger.info(f"Demo {i}/{len(demo_questions)} starting")
            logger.info(f"Type: {description}")
            logger.info(f"Question: {question}")
            logger.info("=" * 30)
            
            demo_start_time = time.time()
            result = pipeline.ask(question)
            demo_time = time.time() - demo_start_time
            
            # ç»Ÿè®¡ä¿¡æ¯æ›´æ–°
            demo_stats["total_execution_time"] += demo_time
            demo_stats["total_tables_used"].update(result['relevant_schemas'])
            
            if result['query_success']:
                demo_stats["successful_queries"] += 1
                logger.info(f"Demo {i} completed successfully")
            elif result.get('schema_insufficient', False):
                demo_stats["failed_queries"] += 1
                demo_stats.setdefault("schema_insufficient_queries", 0)
                demo_stats["schema_insufficient_queries"] += 1
                logger.warning(f"Demo {i} rejected due to insufficient DDL")
            else:
                demo_stats["failed_queries"] += 1
                logger.error(f"Demo {i} execution failed")
            
            # è®°å½•æ¼”ç¤ºç»Ÿè®¡
            demo_stat = {
                "demo_number": i,
                "description": description,
                "success": result['query_success'],
                "schema_insufficient": result.get('schema_insufficient', False),
                "tables_used": len(result['relevant_schemas']),
                "table_names": result['relevant_schemas'],
                "data_records": len(result['data']) if result['query_success'] else 0,
                "execution_time": demo_time,
                "performance": result.get('performance', {})
            }
            demo_stats["performance_breakdown"].append(demo_stat)
            
            logger.info(f"Demo {i} statistics:")
            logger.info(f"   Success: {result['query_success']}")
            logger.info(f"   Tables used: {len(result['relevant_schemas'])}")
            logger.info(f"   Data records: {len(result['data']) if result['query_success'] else 0}")
            logger.info(f"   Execution time: {demo_time:.2f}s")
            if result.get('performance'):
                perf = result['performance']
                logger.info(f"   Retrieval: {perf.get('retrieval_time', 0):.2f}s")
                logger.info(f"   SQL Generation: {perf.get('sql_generation_time', 0):.2f}s") 
                logger.info(f"   Query Execution: {perf.get('execution_time', 0):.2f}s")
                logger.info(f"   Answer Generation: {perf.get('answer_generation_time', 0):.2f}s")
            
            print("-" * 80)
            print("Analysis Results:")
            print("-" * 80)
            print(f"AI identified relevant tables: {', '.join(result['relevant_schemas'])}")
            print(f"Tables involved: {len(result['relevant_schemas'])}")
            print(f"\nGenerated SQL query:")
            print(f"```sql")
            print(result['sql_query'])
            print(f"```")
            
            if result['query_success']:
                print(f"\nQuery executed successfully, returned {len(result['data'])} results")
                print(f"\nAI Analysis:")
                print("=" * 50)
                print(result['answer'])
                print("=" * 50)
                
                if result['data']:
                    print(f"\nKey Data Summary ({len(result['data'])} records):")
                    for idx, record in enumerate(result['data'][:5], 1):
                        print(f"  {idx}. {record}")
                    if len(result['data']) > 5:
                        print(f"  ... and {len(result['data']) - 5} more records")
            else:
                print(f"\nQuery execution encountered challenges: {result.get('query_error', result.get('answer', 'Unknown error'))}")
                print("This type of complex query needs further optimization of SQL generation strategy")
            
            print("=" * 90)
            if i < len(demo_questions):
                print("Preparing next challenge... (2s)")
                import time
                time.sleep(2)
        
        # æœ€ç»ˆç»Ÿè®¡
        total_demo_time = time.time() - total_start_time
        logger.info("=" * 30)
        logger.info("Enterprise demo final statistics")
        logger.info("=" * 30)
        logger.info(f"Total questions: {demo_stats['total_questions']}")
        logger.info(f"Successful queries: {demo_stats['successful_queries']}")
        logger.info(f"Failed queries: {demo_stats['failed_queries']}")
        if demo_stats.get('schema_insufficient_queries', 0) > 0:
            logger.info(f"DDL insufficient rejections: {demo_stats['schema_insufficient_queries']}")
        logger.info(f"Success rate: {demo_stats['successful_queries']/demo_stats['total_questions']*100:.1f}%")
        logger.info(f"Total tables involved: {len(demo_stats['total_tables_used'])}")
        logger.info(f"Tables used: {sorted(list(demo_stats['total_tables_used']))}")
        logger.info(f"Total demo time: {total_demo_time:.2f}s")
        logger.info(f"Average time per question: {total_demo_time/demo_stats['total_questions']:.2f}s")
        logger.info("Average performance metrics:")
        logger.info(f"   Retrieval: {sum(stat['performance'].get('retrieval_time', 0) for stat in demo_stats['performance_breakdown'])/demo_stats['total_questions']:.2f}s")
        logger.info(f"   SQL Generation: {sum(stat['performance'].get('sql_generation_time', 0) for stat in demo_stats['performance_breakdown'])/demo_stats['total_questions']:.2f}s")
        logger.info(f"   Query Execution: {sum(stat['performance'].get('execution_time', 0) for stat in demo_stats['performance_breakdown'])/demo_stats['total_questions']:.2f}s")
        logger.info(f"   Answer Generation: {sum(stat['performance'].get('answer_generation_time', 0) for stat in demo_stats['performance_breakdown'])/demo_stats['total_questions']:.2f}s")
        logger.info("=" * 30)
        
        print("\n" + "=" * 60)
        print("Enterprise Complex Multi-table Query Demo Completed!")
        print("=" * 60)
        
        print("\nAI demonstrated powerful capabilities in enterprise data analysis:")
        print("   âœ“ Intelligent association analysis of 10 core business tables")
        print("   âœ“ Automatic SQL generation for complex business logic")
        print("   âœ“ Multi-dimensional data aggregation and deep insights")
        print("   âœ“ Full-stack capability from simple queries to complex analysis")
        print("   âœ“ Precise understanding and processing of Chinese business scenarios")
        
        print(f"\nTechnical Achievement Statistics:")
        print(f"   Demo query count: {demo_stats['total_questions']} complex scenarios")
        print(f"   Query success rate: {demo_stats['successful_queries']}/{demo_stats['total_questions']} ({demo_stats['successful_queries']/demo_stats['total_questions']*100:.1f}%)")
        print(f"   Data tables involved: {len(demo_stats['total_tables_used'])} enterprise core tables")
        print(f"   Average table associations: {sum(len(stat['table_names']) for stat in demo_stats['performance_breakdown'])/demo_stats['total_questions']:.1f} tables/query")
        print(f"   AI capability demonstration: Vector retrieval + SQL generation + Natural language understanding")
        print(f"   Total time: {total_demo_time:.1f}s (average {total_demo_time/demo_stats['total_questions']:.1f}s/question)")
        
        print("\nThis is the real power of next-generation enterprise Chinese NL2SQL systems!")

    except (ValueError, ImportError) as e:
        print(f"\nâŒ è®¾ç½®è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
    except Exception as e:
        logger.error("An unexpected error occurred during the demo.", exc_info=True)
        print(f"\nâŒ å‘ç”Ÿæ„å¤–é”™è¯¯: {e}")

if __name__ == "__main__":
    run_demo()