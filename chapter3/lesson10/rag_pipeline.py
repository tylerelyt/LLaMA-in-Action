#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
BGE-m3 å¬å› + BGE-reranker é‡æ’ + LLM ç”Ÿæˆç­”æ¡ˆçš„ RAG ç¤ºä¾‹

æœ¬ç¤ºä¾‹å±•ç¤ºäº†ç°ä»£RAGç³»ç»Ÿçš„å®Œæ•´æµç¨‹ï¼š
1. ä½¿ç”¨BGE-m3è¿›è¡Œè¯­ä¹‰å¬å›
2. ä½¿ç”¨BGE-rerankerè¿›è¡Œç²¾ç¡®é‡æ’
3. ä½¿ç”¨LLMåŸºäºé‡æ’åçš„ä¸Šä¸‹æ–‡ç”Ÿæˆæœ€ç»ˆç­”æ¡ˆ

æŠ€æœ¯æ ˆï¼š
- BGE-m3: å¤šè¯­è¨€å¤šç²’åº¦åµŒå…¥æ¨¡å‹
- BGE-reranker: ç²¾ç¡®é‡æ’åºæ¨¡å‹  
- DashScope: é€šä¹‰åƒé—®LLMæœåŠ¡
"""

import os
import json
import time
import numpy as np
from typing import List, Dict, Tuple, Any, Optional
import logging
from dataclasses import dataclass, field
from pathlib import Path

# è®¾ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    from FlagEmbedding import BGEM3FlagModel, FlagReranker
    import dashscope
    from dashscope import Generation
    from langchain_text_splitters import RecursiveCharacterTextSplitter
except ImportError as e:
    logger.error(f"ç¼ºå°‘å¿…è¦çš„ä¾èµ–åŒ…: {e}")
    logger.error("è¯·å®‰è£…: pip install FlagEmbedding dashscope numpy langchain langchain-text-splitters")
    exit(1)

@dataclass
class Document:
    """æ–‡æ¡£æ•°æ®ç»“æ„"""
    id: str
    title: str
    content: str
    metadata: Optional[Dict[str, Any]] = field(default=None)
    chunk_id: Optional[str] = field(default=None)  # æ–‡æ¡£å—ID
    parent_id: Optional[str] = field(default=None)  # çˆ¶æ–‡æ¡£ID
    chunk_index: Optional[int] = field(default=None)  # å—ç´¢å¼•

@dataclass
class RetrievalResult:
    """æ£€ç´¢ç»“æœæ•°æ®ç»“æ„"""
    document: Document
    score: float
    rank: int

class BGERetrievalSystem:
    """åŸºäºBGE-m3çš„æ£€ç´¢ç³»ç»Ÿ"""
    
    def __init__(self, model_path: str = "BAAI/bge-m3"):
        """
        åˆå§‹åŒ–BGE-m3æ£€ç´¢ç³»ç»Ÿ
        
        Args:
            model_path: BGE-m3æ¨¡å‹è·¯å¾„
        """
        logger.info(f"Loading BGE-m3 model: {model_path}")
        try:
            self.model = BGEM3FlagModel(model_path, use_fp16=True)
            logger.info("BGE-m3 model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load BGE-m3 model: {e}")
            raise
            
        self.documents: List[Document] = []
        self.embeddings: Optional[np.ndarray] = None
        
    def add_documents(self, documents: List[Document]):
        """æ·»åŠ æ–‡æ¡£åˆ°æ£€ç´¢ç³»ç»Ÿï¼ŒåŒ…å«æ–‡æ¡£åˆ‡ç‰‡"""
        logger.info(f"Adding {len(documents)} documents to retrieval system")
        
        # åˆå§‹åŒ–æ–‡æ¡£åˆ‡ç‰‡å™¨
        text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=1000,  # æ¯ä¸ªå—çš„æœ€å¤§å­—ç¬¦æ•°
            chunk_overlap=200,  # å—ä¹‹é—´çš„é‡å å­—ç¬¦æ•°
            length_function=len,
            separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", ";", "ï¼›", ":", "ï¼š", ".", " ", ""]
        )
        
        # å­˜å‚¨æ‰€æœ‰æ–‡æ¡£å—
        all_chunks = []
        
        for doc in documents:
            logger.info(f"Splitting document: {doc.title}")
            
            # å¯¹æ–‡æ¡£å†…å®¹è¿›è¡Œåˆ‡ç‰‡
            chunks = text_splitter.split_text(doc.content)
            
            for i, chunk_content in enumerate(chunks):
                chunk_doc = Document(
                    id=f"{doc.id}_chunk_{i}",
                    title=doc.title,  # ä¿æŒåŸæ ‡é¢˜
                    content=chunk_content,
                    metadata=doc.metadata,
                    chunk_id=f"{doc.id}_chunk_{i}",
                    parent_id=doc.id,
                    chunk_index=i
                )
                all_chunks.append(chunk_doc)
        
        logger.info(f"Created {len(all_chunks)} chunks from {len(documents)} documents")
        self.documents = all_chunks
        
        # æ„å»ºæ–‡æ¡£æ–‡æœ¬ç”¨äºåµŒå…¥
        doc_texts = []
        for doc in self.documents:
            # ç»“åˆæ ‡é¢˜å’Œå†…å®¹ï¼Œä½†å†…å®¹å·²ç»æ˜¯åˆ‡ç‰‡åçš„
            full_text = f"{doc.title}\n\n{doc.content}"
            doc_texts.append(full_text)
        
        # ç”ŸæˆåµŒå…¥å‘é‡
        logger.info("Generating embeddings...")
        start_time = time.time()
        embedding_result = self.model.encode(
            doc_texts,
            batch_size=12,
            max_length=8192,
            return_dense=True,
            return_sparse=False,
            return_colbert_vecs=False
        )
        
        # å¤„ç†BGE-m3çš„è¿”å›ç»“æœ
        if isinstance(embedding_result, dict) and 'dense_vecs' in embedding_result:
            self.embeddings = np.array(embedding_result['dense_vecs'])
        else:
            # å¦‚æœç›´æ¥è¿”å›å‘é‡æ•°ç»„
            self.embeddings = np.array(embedding_result)
        
        embedding_time = time.time() - start_time
        logger.info(f"Generated embeddings for {len(documents)} documents in {embedding_time:.2f}s")
        
    def search(self, query: str, top_k: int = 10) -> List[RetrievalResult]:
        """
        æ£€ç´¢ç›¸å…³æ–‡æ¡£
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            top_k: è¿”å›çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            æ£€ç´¢ç»“æœåˆ—è¡¨
        """
        if not self.documents or self.embeddings is None:
            logger.warning("No documents or embeddings available")
            return []
            
        logger.info(f"Searching for query: {query[:50]}...")
        
        # å¯¹æŸ¥è¯¢è¿›è¡ŒåµŒå…¥
        start_time = time.time()
        query_result = self.model.encode(
            [query],
            batch_size=1,
            max_length=8192,
            return_dense=True,
            return_sparse=False,
            return_colbert_vecs=False
        )
        
        # å¤„ç†æŸ¥è¯¢åµŒå…¥ç»“æœ
        if isinstance(query_result, dict) and 'dense_vecs' in query_result:
            query_embedding = np.array(query_result['dense_vecs'][0])
        elif isinstance(query_result, (list, np.ndarray)):
            query_embedding = np.array(query_result[0])
        else:
            query_embedding = np.array(query_result)
        
        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = np.dot(self.embeddings, query_embedding)
        
        # è·å–top_kç»“æœ
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        search_time = time.time() - start_time
        logger.info(f"Search completed in {search_time:.3f}s")
        
        results = []
        for rank, idx in enumerate(top_indices):
            result = RetrievalResult(
                document=self.documents[idx],
                score=float(similarities[idx]),
                rank=rank + 1
            )
            results.append(result)
            
        return results

class BGEReranker:
    """åŸºäºBGE-rerankerçš„é‡æ’åºç³»ç»Ÿ"""
    
    def __init__(self, model_path: str = "BAAI/bge-reranker-v2-m3"):
        """
        åˆå§‹åŒ–BGEé‡æ’åºå™¨
        
        Args:
            model_path: BGE-rerankeræ¨¡å‹è·¯å¾„
        """
        logger.info(f"Loading BGE-reranker model: {model_path}")
        try:
            self.reranker = FlagReranker(model_path, use_fp16=True)
            logger.info("BGE-reranker model loaded successfully")
        except Exception as e:
            logger.error(f"Failed to load BGE-reranker model: {e}")
            raise
    
    def rerank(self, query: str, results: List[RetrievalResult], top_k: int = 5) -> List[RetrievalResult]:
        """
        é‡æ’åºæ£€ç´¢ç»“æœ
        
        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            results: åˆå§‹æ£€ç´¢ç»“æœ
            top_k: è¿”å›çš„é‡æ’åç»“æœæ•°é‡
            
        Returns:
            é‡æ’åçš„ç»“æœåˆ—è¡¨
        """
        if not results:
            return []
            
        logger.info(f"Reranking {len(results)} results...")
        
        # å‡†å¤‡è¾“å…¥å¯¹
        sentence_pairs = []
        for result in results:
            # å¯¹äºåˆ‡ç‰‡åçš„æ–‡æ¡£ï¼Œä½¿ç”¨å—å†…å®¹è¿›è¡Œé‡æ’åº
            doc_text = f"{result.document.title}\n\n{result.document.content}"
            # å¦‚æœæ˜¯æ–‡æ¡£å—ï¼Œå¯ä»¥æ·»åŠ å—ä¿¡æ¯
            if result.document.chunk_id:
                doc_text = f"{result.document.title} (å— {result.document.chunk_index})\n\n{result.document.content}"
            sentence_pairs.append([query, doc_text])
        
        # è¿›è¡Œé‡æ’åº
        start_time = time.time()
        scores = self.reranker.compute_score(sentence_pairs, batch_size=8)
        rerank_time = time.time() - start_time
        
        logger.info(f"Reranking completed in {rerank_time:.3f}s")
        
        # åˆ›å»ºæ–°çš„ç»“æœåˆ—è¡¨
        reranked_results = []
        for i, score in enumerate(scores):
            result = results[i]
            reranked_result = RetrievalResult(
                document=result.document,
                score=float(score),
                rank=i + 1
            )
            reranked_results.append(reranked_result)
        
        # æŒ‰é‡æ’åºåˆ†æ•°æ’åº
        reranked_results.sort(key=lambda x: x.score, reverse=True)
        
        # æ›´æ–°æ’å
        for i, result in enumerate(reranked_results[:top_k]):
            result.rank = i + 1
            
        return reranked_results[:top_k]

class LLMGenerator:
    """åŸºäºDashScopeçš„ç­”æ¡ˆç”Ÿæˆå™¨"""
    
    def __init__(self, api_key: Optional[str] = None, model: str = "qwen-max"):
        """
        åˆå§‹åŒ–LLMç”Ÿæˆå™¨
        
        Args:
            api_key: DashScope APIå¯†é’¥
            model: æ¨¡å‹åç§°
        """
        self.api_key = api_key or os.getenv('DASHSCOPE_API_KEY')
        self.model = model
        
        if not self.api_key:
            raise ValueError("è¯·è®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡æˆ–ä¼ å…¥api_keyå‚æ•°")
            
        dashscope.api_key = self.api_key
        logger.info(f"LLM Generator initialized with model: {model}")
    
    def generate_answer(self, query: str, contexts: List[RetrievalResult]) -> Dict[str, Any]:
        """
        åŸºäºæ£€ç´¢ä¸Šä¸‹æ–‡ç”Ÿæˆç­”æ¡ˆ
        
        Args:
            query: ç”¨æˆ·æŸ¥è¯¢
            contexts: é‡æ’åçš„ä¸Šä¸‹æ–‡æ–‡æ¡£
            
        Returns:
            ç”Ÿæˆç»“æœåŒ…å«ç­”æ¡ˆå’Œå…ƒæ•°æ®
        """
        if not contexts:
            return {
                "answer": "æŠ±æ­‰ï¼Œæˆ‘æ²¡æœ‰æ‰¾åˆ°ç›¸å…³çš„ä¿¡æ¯æ¥å›ç­”æ‚¨çš„é—®é¢˜ã€‚",
                "sources": [],
                "confidence": 0.0
            }
        
        # æ„å»ºä¸Šä¸‹æ–‡æ–‡æœ¬
        context_texts = []
        sources = []
        
        for i, result in enumerate(contexts):
            doc = result.document
            
            # æ„å»ºå‚è€ƒèµ„æ–™ä¿¡æ¯ï¼ŒåŒ…å«å—ä¿¡æ¯
            title_info = doc.title
            if doc.chunk_id and doc.chunk_index is not None:
                title_info = f"{doc.title} (ç¬¬{doc.chunk_index + 1}å—)"
            
            context_texts.append(
                f"å‚è€ƒèµ„æ–™{i+1}ï¼š\n"
                f"æ ‡é¢˜ï¼š{title_info}\n"
                f"å†…å®¹ï¼š{doc.content}\n"
                f"ç›¸å…³æ€§è¯„åˆ†ï¼š{result.score:.3f}\n"
            )
            sources.append({
                "title": title_info,
                "score": result.score,
                "rank": result.rank,
                "chunk_id": doc.chunk_id,
                "parent_id": doc.parent_id
            })
        
        context_str = "\n" + "="*50 + "\n".join(context_texts)
        
        # æ„å»ºæç¤ºè¯
        prompt = f"""ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„AIåŠ©æ‰‹ï¼Œè¯·åŸºäºæä¾›çš„å‚è€ƒèµ„æ–™å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜ï¼š{query}

å‚è€ƒèµ„æ–™ï¼š
{context_str}

è¯·æŒ‰ç…§ä»¥ä¸‹è¦æ±‚å›ç­”ï¼š
1. åŸºäºå‚è€ƒèµ„æ–™ä¸­çš„ä¿¡æ¯è¿›è¡Œå›ç­”ï¼Œç¡®ä¿å‡†ç¡®æ€§
2. å¦‚æœå‚è€ƒèµ„æ–™ä¸­æ²¡æœ‰ç›´æ¥ç­”æ¡ˆï¼Œè¯·è¯šå®è¯´æ˜
3. åœ¨å›ç­”ä¸­é€‚å½“å¼•ç”¨å‚è€ƒèµ„æ–™çš„å…³é”®ä¿¡æ¯
4. ä¿æŒå›ç­”çš„é€»è¾‘æ€§å’Œæ¡ç†æ€§
5. ä½¿ç”¨ç®€æ´æ˜äº†çš„è¯­è¨€

å›ç­”ï¼š"""

        logger.info("Generating answer with LLM...")
        start_time = time.time()
        
        try:
            response = Generation.call(
                model=self.model,
                prompt=prompt,
                max_tokens=2000,
                temperature=0.3,
                top_p=0.8,
                repetition_penalty=1.05
            )
            
            generation_time = time.time() - start_time
            logger.info(f"Answer generated in {generation_time:.2f}s")
            
            # å¤„ç†DashScopeå“åº”
            if hasattr(response, 'status_code') and response.status_code == 200:
                answer = response.output.text.strip()
                
                # è¯„ä¼°ç½®ä¿¡åº¦ï¼ˆåŸºäºä¸Šä¸‹æ–‡ç›¸å…³æ€§ï¼‰
                avg_score = sum(r.score for r in contexts) / len(contexts)
                confidence = min(avg_score * 0.8, 0.95)  # å½’ä¸€åŒ–åˆ°åˆç†èŒƒå›´
                
                return {
                    "answer": answer,
                    "sources": sources,
                    "confidence": confidence,
                    "generation_time": generation_time,
                    "context_count": len(contexts)
                }
            else:
                error_msg = getattr(response, 'message', 'Unknown error')
                logger.error(f"LLM generation failed: {error_msg}")
                return {
                    "answer": "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                    "sources": sources,
                    "confidence": 0.0
                }
                
        except Exception as e:
            logger.error(f"LLM generation error: {e}")
            return {
                "answer": "æŠ±æ­‰ï¼Œç”Ÿæˆç­”æ¡ˆæ—¶å‡ºç°é”™è¯¯ï¼Œè¯·ç¨åé‡è¯•ã€‚",
                "sources": sources,
                "confidence": 0.0
            }

class RAGPipeline:
    """å®Œæ•´çš„RAGæµæ°´çº¿"""
    
    def __init__(self, 
                 retrieval_model: str = "BAAI/bge-m3",
                 reranker_model: str = "BAAI/bge-reranker-v2-m3",
                 llm_model: str = "qwen-max"):
        """
        åˆå§‹åŒ–RAGæµæ°´çº¿
        
        Args:
            retrieval_model: æ£€ç´¢æ¨¡å‹è·¯å¾„
            reranker_model: é‡æ’åºæ¨¡å‹è·¯å¾„
            llm_model: LLMæ¨¡å‹åç§°
        """
        logger.info("Initializing RAG Pipeline...")
        
        self.retrieval_system = BGERetrievalSystem(retrieval_model)
        self.reranker = BGEReranker(reranker_model)
        self.llm_generator = LLMGenerator(model=llm_model)
        
        logger.info("RAG Pipeline initialized successfully")
    
    def load_documents(self, documents: List[Document]):
        """åŠ è½½æ–‡æ¡£åˆ°æ£€ç´¢ç³»ç»Ÿ"""
        self.retrieval_system.add_documents(documents)
    
    def query(self, 
             question: str,
             retrieval_top_k: int = 20,
             rerank_top_k: int = 5) -> Dict[str, Any]:
        """
        æ‰§è¡Œå®Œæ•´çš„RAGæŸ¥è¯¢æµç¨‹
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            retrieval_top_k: æ£€ç´¢é˜¶æ®µè¿”å›çš„æ–‡æ¡£æ•°é‡
            rerank_top_k: é‡æ’åºåä¿ç•™çš„æ–‡æ¡£æ•°é‡
            
        Returns:
            å®Œæ•´çš„æŸ¥è¯¢ç»“æœ
        """
        logger.info(f"Processing RAG query: {question[:50]}...")
        total_start_time = time.time()
        
        # æ­¥éª¤1: BGE-m3æ£€ç´¢
        logger.info("Step 1: BGE-m3 Retrieval...")
        retrieval_results = self.retrieval_system.search(question, retrieval_top_k)
        
        if not retrieval_results:
            return {
                "question": question,
                "answer": "æŠ±æ­‰ï¼Œæ²¡æœ‰æ‰¾åˆ°ç›¸å…³æ–‡æ¡£ã€‚",
                "sources": [],
                "confidence": 0.0,
                "pipeline_stats": {
                    "retrieval_count": 0,
                    "rerank_count": 0,
                    "total_time": time.time() - total_start_time
                }
            }
        
        logger.info(f"Retrieved {len(retrieval_results)} documents")
        
        # æ­¥éª¤2: BGE-rerankeré‡æ’åº
        logger.info("Step 2: BGE-reranker Reranking...")
        reranked_results = self.reranker.rerank(question, retrieval_results, rerank_top_k)
        logger.info(f"Reranked to top {len(reranked_results)} documents")
        
        # æ­¥éª¤3: LLMç”Ÿæˆç­”æ¡ˆ
        logger.info("Step 3: LLM Answer Generation...")
        generation_result = self.llm_generator.generate_answer(question, reranked_results)
        
        total_time = time.time() - total_start_time
        
        # æ•´åˆç»“æœ
        result = {
            "question": question,
            "answer": generation_result["answer"],
            "sources": generation_result["sources"],
            "confidence": generation_result["confidence"],
            "pipeline_stats": {
                "retrieval_count": len(retrieval_results),
                "rerank_count": len(reranked_results),
                "total_time": total_time,
                "generation_time": generation_result.get("generation_time", 0)
            }
        }
        
        logger.info(f"RAG query completed in {total_time:.2f}s")
        return result

def load_sample_documents() -> List[Document]:
    """åŠ è½½ç¤ºä¾‹æ–‡æ¡£æ•°æ®"""
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç¤ºä¾‹æ•°æ®æ–‡ä»¶
    data_file = Path("sample_documents.json")
    if data_file.exists():
        logger.info("Loading documents from sample_documents.json")
        with open(data_file, 'r', encoding='utf-8') as f:
            doc_data = json.load(f)
            return [Document(**doc) for doc in doc_data]
    
    # å¦‚æœæ²¡æœ‰æ•°æ®æ–‡ä»¶ï¼Œä½¿ç”¨å†…ç½®ç¤ºä¾‹
    logger.info("Using built-in sample documents")
    
    sample_docs = [
        Document(
            id="doc_001",
            title="Pythonç¼–ç¨‹åŸºç¡€ - å˜é‡å’Œæ•°æ®ç±»å‹",
            content="""Pythonæ˜¯ä¸€ç§é«˜çº§ç¼–ç¨‹è¯­è¨€ï¼Œå…·æœ‰ç®€æ´çš„è¯­æ³•å’Œå¼ºå¤§çš„åŠŸèƒ½ã€‚åœ¨Pythonä¸­ï¼Œå˜é‡ç”¨äºå­˜å‚¨æ•°æ®ï¼Œä¸éœ€è¦å£°æ˜å˜é‡ç±»å‹ã€‚Pythonæ”¯æŒå¤šç§æ•°æ®ç±»å‹ï¼š

1. æ•°å­—ç±»å‹ï¼š
   - intï¼ˆæ•´æ•°ï¼‰ï¼šå¦‚ 42, -17, 0
   - floatï¼ˆæµ®ç‚¹æ•°ï¼‰ï¼šå¦‚ 3.14, -0.001, 2.0
   - complexï¼ˆå¤æ•°ï¼‰ï¼šå¦‚ 3+4j, 1-2j

2. å­—ç¬¦ä¸²ç±»å‹ï¼ˆstrï¼‰ï¼š
   - ä½¿ç”¨å•å¼•å·æˆ–åŒå¼•å·å®šä¹‰ï¼š'hello' æˆ– "world"
   - æ”¯æŒè½¬ä¹‰å­—ç¬¦ï¼š\nï¼ˆæ¢è¡Œï¼‰, \tï¼ˆåˆ¶è¡¨ç¬¦ï¼‰
   - æ”¯æŒæ ¼å¼åŒ–ï¼šf"Hello {name}"

3. å¸ƒå°”ç±»å‹ï¼ˆboolï¼‰ï¼š
   - True å’Œ False
   - é€šå¸¸ç”¨äºæ¡ä»¶åˆ¤æ–­

4. å®¹å™¨ç±»å‹ï¼š
   - listï¼ˆåˆ—è¡¨ï¼‰ï¼šæœ‰åºå¯å˜åºåˆ— [1, 2, 3]
   - tupleï¼ˆå…ƒç»„ï¼‰ï¼šæœ‰åºä¸å¯å˜åºåˆ— (1, 2, 3)
   - dictï¼ˆå­—å…¸ï¼‰ï¼šé”®å€¼å¯¹æ˜ å°„ {'key': 'value'}
   - setï¼ˆé›†åˆï¼‰ï¼šæ— åºå”¯ä¸€å…ƒç´ é›†åˆ {1, 2, 3}

å˜é‡èµ‹å€¼å¾ˆç®€å•ï¼šx = 10, name = "Python", data = [1, 2, 3]"""
        ),
        
        Document(
            id="doc_002", 
            title="Pythonç¼–ç¨‹è¿›é˜¶ - å‡½æ•°å’Œæ¨¡å—",
            content="""å‡½æ•°æ˜¯Pythonä¸­ç»„ç»‡ä»£ç çš„é‡è¦æ–¹å¼ï¼Œå¯ä»¥æé«˜ä»£ç çš„å¤ç”¨æ€§å’Œå¯ç»´æŠ¤æ€§ã€‚

å‡½æ•°å®šä¹‰è¯­æ³•ï¼š
```python
def function_name(parameters):
    \"\"\"å‡½æ•°è¯´æ˜æ–‡æ¡£\"\"\"
    # å‡½æ•°ä½“
    return result  # å¯é€‰
```

å‡½æ•°ç‰¹æ€§ï¼š
1. å‚æ•°ç±»å‹ï¼š
   - ä½ç½®å‚æ•°ï¼šdef func(a, b)
   - é»˜è®¤å‚æ•°ï¼šdef func(a, b=10)
   - å¯å˜å‚æ•°ï¼šdef func(*args, **kwargs)
   - å…³é”®å­—å‚æ•°ï¼šdef func(a, *, b, c)

2. è¿”å›å€¼ï¼š
   - å¯ä»¥è¿”å›å•ä¸ªå€¼ã€å¤šä¸ªå€¼ï¼ˆå…ƒç»„ï¼‰
   - æ²¡æœ‰returnè¯­å¥æ—¶è¿”å›None

3. ä½œç”¨åŸŸï¼š
   - å±€éƒ¨ä½œç”¨åŸŸï¼šå‡½æ•°å†…éƒ¨å˜é‡
   - å…¨å±€ä½œç”¨åŸŸï¼šæ¨¡å—çº§å˜é‡
   - é—­åŒ…ï¼šå†…å±‚å‡½æ•°è®¿é—®å¤–å±‚å‡½æ•°å˜é‡

æ¨¡å—ç³»ç»Ÿï¼š
- æ¨¡å—æ˜¯åŒ…å«Pythonä»£ç çš„æ–‡ä»¶ï¼ˆ.pyæ–‡ä»¶ï¼‰
- ä½¿ç”¨importè¯­å¥å¯¼å…¥æ¨¡å—ï¼šimport math, from os import path
- åŒ…æ˜¯åŒ…å«å¤šä¸ªæ¨¡å—çš„ç›®å½•ï¼Œå¿…é¡»æœ‰__init__.pyæ–‡ä»¶
- Pythonæ ‡å‡†åº“æä¾›äº†ä¸°å¯Œçš„å†…ç½®æ¨¡å—ï¼šos, sys, json, datetimeç­‰"""
        ),
        
        Document(
            id="doc_003",
            title="Pythoné¢å‘å¯¹è±¡ç¼–ç¨‹ - ç±»å’Œå¯¹è±¡",
            content="""é¢å‘å¯¹è±¡ç¼–ç¨‹ï¼ˆOOPï¼‰æ˜¯Pythonçš„é‡è¦ç‰¹æ€§ï¼Œé€šè¿‡ç±»å’Œå¯¹è±¡æ¥ç»„ç»‡ä»£ç ã€‚

ç±»å®šä¹‰è¯­æ³•ï¼š
```python
class ClassName:
    \"\"\"ç±»è¯´æ˜æ–‡æ¡£\"\"\"
    
    class_variable = "ç±»å˜é‡"
    
    def __init__(self, parameters):
        \"\"\"æ„é€ æ–¹æ³•\"\"\"
        self.instance_variable = parameters
    
    def method_name(self):
        \"\"\"å®ä¾‹æ–¹æ³•\"\"\"
        return self.instance_variable
```

OOPæ ¸å¿ƒæ¦‚å¿µï¼š

1. å°è£…ï¼ˆEncapsulationï¼‰ï¼š
   - å°†æ•°æ®å’Œæ–¹æ³•ç»„åˆåœ¨ç±»ä¸­
   - ä½¿ç”¨ç§æœ‰å±æ€§ï¼ˆ_variableï¼‰å’Œæ–¹æ³•ï¼ˆ_methodï¼‰
   - æä¾›å…¬å…±æ¥å£è®¿é—®å†…éƒ¨æ•°æ®

2. ç»§æ‰¿ï¼ˆInheritanceï¼‰ï¼š
   - å­ç±»ç»§æ‰¿çˆ¶ç±»çš„å±æ€§å’Œæ–¹æ³•ï¼šclass Child(Parent)
   - æ–¹æ³•é‡å†™ï¼šåœ¨å­ç±»ä¸­é‡æ–°å®šä¹‰çˆ¶ç±»æ–¹æ³•
   - super()å‡½æ•°ï¼šè°ƒç”¨çˆ¶ç±»æ–¹æ³•

3. å¤šæ€ï¼ˆPolymorphismï¼‰ï¼š
   - ç›¸åŒæ¥å£ï¼Œä¸åŒå®ç°
   - é¸­å­ç±»å‹ï¼šå¦‚æœå®ƒçœ‹èµ·æ¥åƒé¸­å­ï¼Œå«èµ·æ¥åƒé¸­å­ï¼Œé‚£å®ƒå°±æ˜¯é¸­å­

ç‰¹æ®Šæ–¹æ³•ï¼ˆé­”æœ¯æ–¹æ³•ï¼‰ï¼š
- __init__ï¼šæ„é€ æ–¹æ³•
- __str__ï¼šå­—ç¬¦ä¸²è¡¨ç¤º
- __len__ï¼šé•¿åº¦
- __getitem__ï¼šç´¢å¼•è®¿é—®
- __add__ï¼šåŠ æ³•è¿ç®—ç¬¦é‡è½½"""
        ),
        
        Document(
            id="doc_004",
            title="Pythonæ•°æ®å¤„ç† - NumPyå’ŒPandas",
            content="""NumPyå’ŒPandasæ˜¯Pythonæ•°æ®ç§‘å­¦ç”Ÿæ€ç³»ç»Ÿçš„æ ¸å¿ƒåº“ã€‚

NumPyï¼ˆNumerical Pythonï¼‰ï¼š
- æä¾›é«˜æ€§èƒ½çš„å¤šç»´æ•°ç»„å¯¹è±¡ndarray
- æ”¯æŒå¹¿æ’­ï¼ˆbroadcastingï¼‰æœºåˆ¶
- ä¸°å¯Œçš„æ•°å­¦å‡½æ•°åº“

NumPyåŸºæœ¬ç”¨æ³•ï¼š
```python
import numpy as np

# åˆ›å»ºæ•°ç»„
arr = np.array([1, 2, 3, 4, 5])
matrix = np.array([[1, 2], [3, 4]])

# æ•°ç»„æ“ä½œ
arr * 2  # å…ƒç´ ä¹˜æ³•
np.sum(arr)  # æ±‚å’Œ
np.mean(arr)  # å¹³å‡å€¼
```

Pandasï¼š
- æä¾›DataFrameå’ŒSeriesæ•°æ®ç»“æ„
- å¼ºå¤§çš„æ•°æ®è¯»å–ã€æ¸…æ´—ã€åˆ†æåŠŸèƒ½
- æ”¯æŒå¤šç§æ•°æ®æ ¼å¼ï¼šCSV, Excel, JSON, SQLç­‰

PandasåŸºæœ¬ç”¨æ³•ï¼š
```python
import pandas as pd

# åˆ›å»ºDataFrame
df = pd.DataFrame({
    'name': ['Alice', 'Bob', 'Charlie'],
    'age': [25, 30, 35],
    'city': ['Beijing', 'Shanghai', 'Guangzhou']
})

# æ•°æ®æ“ä½œ
df.head()  # æŸ¥çœ‹å‰å‡ è¡Œ
df.describe()  # ç»Ÿè®¡æè¿°
df.groupby('city').mean()  # åˆ†ç»„èšåˆ
```

å¸¸ç”¨æ•°æ®æ“ä½œï¼š
- æ•°æ®é€‰æ‹©ï¼šdf['column'], df.loc[], df.iloc[]
- æ•°æ®è¿‡æ»¤ï¼šdf[df['age'] > 25]
- æ•°æ®åˆå¹¶ï¼špd.merge(), pd.concat()
- æ•°æ®é€è§†ï¼šdf.pivot_table()"""
        ),
        
        Document(
            id="doc_005",
            title="æœºå™¨å­¦ä¹ åŸºç¡€ - Scikit-learnå…¥é—¨",
            content="""Scikit-learnæ˜¯Pythonæœ€æµè¡Œçš„æœºå™¨å­¦ä¹ åº“ï¼Œæä¾›äº†ç®€å•é«˜æ•ˆçš„æ•°æ®æŒ–æ˜å’Œæ•°æ®åˆ†æå·¥å…·ã€‚

ä¸»è¦ç‰¹æ€§ï¼š
- ç»Ÿä¸€çš„APIè®¾è®¡
- ä¸°å¯Œçš„ç®—æ³•æ”¯æŒ
- ä¼˜ç§€çš„æ–‡æ¡£å’Œç¤ºä¾‹
- ä¸NumPyã€Pandasé›†æˆè‰¯å¥½

æœºå™¨å­¦ä¹ å·¥ä½œæµç¨‹ï¼š

1. æ•°æ®å‡†å¤‡ï¼š
```python
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split

# åŠ è½½æ•°æ®
data = load_iris()
X, y = data.data, data.target

# åˆ’åˆ†è®­ç»ƒé›†å’Œæµ‹è¯•é›†
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

2. æ¨¡å‹è®­ç»ƒï¼š
```python
from sklearn.ensemble import RandomForestClassifier

# åˆ›å»ºæ¨¡å‹
model = RandomForestClassifier(n_estimators=100)

# è®­ç»ƒæ¨¡å‹
model.fit(X_train, y_train)
```

3. æ¨¡å‹è¯„ä¼°ï¼š
```python
from sklearn.metrics import accuracy_score, classification_report

# é¢„æµ‹
y_pred = model.predict(X_test)

# è¯„ä¼°
accuracy = accuracy_score(y_test, y_pred)
print(f"å‡†ç¡®ç‡: {accuracy:.3f}")
```

å¸¸ç”¨ç®—æ³•ç±»åˆ«ï¼š
- ç›‘ç£å­¦ä¹ ï¼šåˆ†ç±»ï¼ˆSVM, Random Forestï¼‰ã€å›å½’ï¼ˆLinear Regressionï¼‰
- æ— ç›‘ç£å­¦ä¹ ï¼šèšç±»ï¼ˆK-Meansï¼‰ã€é™ç»´ï¼ˆPCAï¼‰
- æ¨¡å‹é€‰æ‹©ï¼šäº¤å‰éªŒè¯ã€ç½‘æ ¼æœç´¢
- æ•°æ®é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–ã€ç‰¹å¾é€‰æ‹©"""
        ),
        
        Document(
            id="doc_006",
            title="æ·±åº¦å­¦ä¹ æ¡†æ¶ - TensorFlowå’ŒPyTorch",
            content="""TensorFlowå’ŒPyTorchæ˜¯ç›®å‰æœ€ä¸»æµçš„æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚

TensorFlowç‰¹ç‚¹ï¼š
- Googleå¼€å‘çš„å¼€æºæ¡†æ¶
- ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²å‹å¥½
- TensorFlow 2.xé‡‡ç”¨eager execution
- Kerasä½œä¸ºé«˜çº§API

TensorFlowåŸºæœ¬ç”¨æ³•ï¼š
```python
import tensorflow as tf

# åˆ›å»ºç®€å•çš„ç¥ç»ç½‘ç»œ
model = tf.keras.Sequential([
    tf.keras.layers.Dense(128, activation='relu'),
    tf.keras.layers.Dropout(0.2),
    tf.keras.layers.Dense(10, activation='softmax')
])

# ç¼–è¯‘æ¨¡å‹
model.compile(
    optimizer='adam',
    loss='sparse_categorical_crossentropy',
    metrics=['accuracy']
)

# è®­ç»ƒæ¨¡å‹
model.fit(X_train, y_train, epochs=10, batch_size=32)
```

PyTorchç‰¹ç‚¹ï¼š
- Facebookå¼€å‘çš„å¼€æºæ¡†æ¶
- åŠ¨æ€è®¡ç®—å›¾ï¼Œæ›´çµæ´»
- ç ”ç©¶å‹å¥½ï¼Œè°ƒè¯•æ–¹ä¾¿
- å¼ºå¤§çš„è‡ªåŠ¨å¾®åˆ†ç³»ç»Ÿ

PyTorchåŸºæœ¬ç”¨æ³•ï¼š
```python
import torch
import torch.nn as nn

# å®šä¹‰ç¥ç»ç½‘ç»œ
class Net(nn.Module):
    def __init__(self):
        super(Net, self).__init__()
        self.fc1 = nn.Linear(784, 128)
        self.fc2 = nn.Linear(128, 10)
    
    def forward(self, x):
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# åˆ›å»ºæ¨¡å‹å’Œä¼˜åŒ–å™¨
model = Net()
optimizer = torch.optim.Adam(model.parameters())
criterion = nn.CrossEntropyLoss()
```

é€‰æ‹©å»ºè®®ï¼š
- ç ”ç©¶å’ŒåŸå‹å¼€å‘ï¼šPyTorchæ›´çµæ´»
- ç”Ÿäº§éƒ¨ç½²ï¼šTensorFlowç”Ÿæ€æ›´å®Œå–„
- å­¦ä¹ æˆæœ¬ï¼šä¸¤è€…éƒ½æœ‰ä¸°å¯Œçš„æ•™ç¨‹å’Œç¤¾åŒºæ”¯æŒ"""
        ),
        
        Document(
            id="doc_007",
            title="Webå¼€å‘æ¡†æ¶ - Flaskå’ŒDjango",
            content="""Flaskå’ŒDjangoæ˜¯Pythonæœ€å—æ¬¢è¿çš„Webå¼€å‘æ¡†æ¶ã€‚

Flaskç‰¹ç‚¹ï¼š
- è½»é‡çº§å¾®æ¡†æ¶
- çµæ´»æ€§é«˜ï¼Œæ‰©å±•æ€§å¼º
- æœ€å°åŒ–æ ¸å¿ƒï¼ŒæŒ‰éœ€æ·»åŠ åŠŸèƒ½
- é€‚åˆå°åˆ°ä¸­å‹é¡¹ç›®

FlaskåŸºæœ¬ç”¨æ³•ï¼š
```python
from flask import Flask, render_template, request

app = Flask(__name__)

@app.route('/')
def home():
    return "Hello, Flask!"

@app.route('/user/<name>')
def user(name):
    return f"Hello, {name}!"

@app.route('/api/data', methods=['POST'])
def api_data():
    data = request.get_json()
    return {'status': 'success', 'data': data}

if __name__ == '__main__':
    app.run(debug=True)
```

Djangoç‰¹ç‚¹ï¼š
- å…¨åŠŸèƒ½Webæ¡†æ¶
- "ç”µæ± å·²åŒ…å«"å“²å­¦
- å¼ºå¤§çš„ORMç³»ç»Ÿ
- è‡ªåŠ¨ç”Ÿæˆç®¡ç†ç•Œé¢
- é€‚åˆå¤§å‹é¡¹ç›®

Djangoæ ¸å¿ƒç»„ä»¶ï¼š
1. æ¨¡å‹ï¼ˆModelsï¼‰ï¼šæ•°æ®å±‚ï¼Œå®šä¹‰æ•°æ®ç»“æ„
2. è§†å›¾ï¼ˆViewsï¼‰ï¼šä¸šåŠ¡é€»è¾‘å±‚ï¼Œå¤„ç†è¯·æ±‚
3. æ¨¡æ¿ï¼ˆTemplatesï¼‰ï¼šè¡¨ç°å±‚ï¼Œç”ŸæˆHTML
4. URLé…ç½®ï¼šè·¯ç”±ç³»ç»Ÿ

DjangoåŸºæœ¬ç”¨æ³•ï¼š
```python
# models.py
from django.db import models

class User(models.Model):
    username = models.CharField(max_length=100)
    email = models.EmailField()
    created_at = models.DateTimeField(auto_now_add=True)

# views.py
from django.shortcuts import render
from django.http import JsonResponse

def user_list(request):
    users = User.objects.all()
    return render(request, 'users.html', {'users': users})
```

é€‰æ‹©å»ºè®®ï¼š
- å¿«é€ŸåŸå‹ï¼šFlaskæ›´ç®€å•
- å¤æ‚åº”ç”¨ï¼šDjangoåŠŸèƒ½æ›´å®Œæ•´
- å­¦ä¹ æ›²çº¿ï¼šFlaskè¾ƒå¹³ç¼“ï¼ŒDjangoè¾ƒé™¡å³­"""
        ),
        
        Document(
            id="doc_008",
            title="æ•°æ®å¯è§†åŒ– - Matplotlibå’ŒPlotly",
            content="""æ•°æ®å¯è§†åŒ–æ˜¯æ•°æ®åˆ†æçš„é‡è¦ç¯èŠ‚ï¼ŒPythonæä¾›äº†å¤šç§ä¼˜ç§€çš„å¯è§†åŒ–åº“ã€‚

Matplotlibï¼š
- Pythonæœ€åŸºç¡€çš„ç»˜å›¾åº“
- åŠŸèƒ½å…¨é¢ï¼Œå¯å®šåˆ¶æ€§å¼º
- æ”¯æŒå¤šç§è¾“å‡ºæ ¼å¼
- è¯­æ³•ç›¸å¯¹å¤æ‚ä½†åŠŸèƒ½å¼ºå¤§

MatplotlibåŸºæœ¬ç”¨æ³•ï¼š
```python
import matplotlib.pyplot as plt
import numpy as np

# åŸºæœ¬çº¿å›¾
x = np.linspace(0, 10, 100)
y = np.sin(x)

plt.figure(figsize=(10, 6))
plt.plot(x, y, label='sin(x)')
plt.xlabel('Xè½´')
plt.ylabel('Yè½´')
plt.title('æ­£å¼¦å‡½æ•°å›¾')
plt.legend()
plt.grid(True)
plt.show()

# å­å›¾
fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))
ax1.plot(x, np.sin(x))
ax2.plot(x, np.cos(x))
```

Plotlyï¼š
- äº¤äº’å¼å¯è§†åŒ–åº“
- æ”¯æŒWebéƒ¨ç½²
- ç¾è§‚çš„é»˜è®¤æ ·å¼
- æ”¯æŒ3Dç»˜å›¾

PlotlyåŸºæœ¬ç”¨æ³•ï¼š
```python
import plotly.graph_objects as go
import plotly.express as px

# ä½¿ç”¨Express APIï¼ˆç®€å•ï¼‰
df = px.data.iris()
fig = px.scatter(df, x="sepal_width", y="sepal_length", 
                color="species", title="é¸¢å°¾èŠ±æ•°æ®æ•£ç‚¹å›¾")
fig.show()

# ä½¿ç”¨Graph Objects APIï¼ˆçµæ´»ï¼‰
fig = go.Figure()
fig.add_trace(go.Scatter(x=[1, 2, 3, 4], y=[10, 11, 12, 13]))
fig.update_layout(title="è‡ªå®šä¹‰å›¾è¡¨")
fig.show()
```

å…¶ä»–å¯è§†åŒ–åº“ï¼š
- Seabornï¼šåŸºäºMatplotlibçš„ç»Ÿè®¡å¯è§†åŒ–
- Bokehï¼šäº¤äº’å¼Webå¯è§†åŒ–
- Altairï¼šåŸºäºVega-Liteçš„å£°æ˜å¼å¯è§†åŒ–

é€‰æ‹©å»ºè®®ï¼š
- é™æ€å›¾è¡¨ï¼šMatplotlib + Seaborn
- äº¤äº’å¼å›¾è¡¨ï¼šPlotlyæˆ–Bokeh
- å¿«é€Ÿæ¢ç´¢ï¼šPandaså†…ç½®ç»˜å›¾åŠŸèƒ½"""
        )
    ]
    
    return sample_docs

def main():
    """ä¸»æ¼”ç¤ºå‡½æ•°"""
    print("=" * 60)
    print("ğŸš€ BGE-m3 + BGE-reranker + LLM RAG æ¼”ç¤º")
    print("=" * 60)
    
    # æ£€æŸ¥APIå¯†é’¥
    if not os.getenv('DASHSCOPE_API_KEY'):
        print("âš ï¸  è¯·è®¾ç½®DASHSCOPE_API_KEYç¯å¢ƒå˜é‡")
        print("   export DASHSCOPE_API_KEY=your_api_key")
        return
    
    try:
        # åˆå§‹åŒ–RAGç³»ç»Ÿ
        print("\nğŸ“‹ åˆå§‹åŒ–RAGç³»ç»Ÿ...")
        rag_pipeline = RAGPipeline()
        
        # åŠ è½½ç¤ºä¾‹æ–‡æ¡£
        print("\nğŸ“š åŠ è½½ç¤ºä¾‹æ–‡æ¡£...")
        documents = load_sample_documents()
        rag_pipeline.load_documents(documents)
        print(f"âœ… å·²åŠ è½½ {len(documents)} ä¸ªæ–‡æ¡£")
        
        # ç¤ºä¾‹æŸ¥è¯¢
        test_queries = [
            "Pythonä¸­æœ‰å“ªäº›æ•°æ®ç±»å‹ï¼Ÿ",
            "å¦‚ä½•åœ¨Pythonä¸­å®šä¹‰å‡½æ•°ï¼Ÿ",
            "Pandaså’ŒNumPyæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ",
            "Flaskå’ŒDjangoå“ªä¸ªæ›´é€‚åˆæ–°æ‰‹ï¼Ÿ",
            "æœºå™¨å­¦ä¹ çš„åŸºæœ¬æµç¨‹æ˜¯ä»€ä¹ˆï¼Ÿ"
        ]
        
        print("\nğŸ” å¼€å§‹RAGæŸ¥è¯¢æ¼”ç¤º...")
        print("=" * 60)
        
        for i, query in enumerate(test_queries[:3], 1):  # æ¼”ç¤ºå‰3ä¸ªæŸ¥è¯¢
            print(f"\nã€æŸ¥è¯¢ {i}ã€‘{query}")
            print("-" * 50)
            
            # æ‰§è¡ŒRAGæŸ¥è¯¢
            result = rag_pipeline.query(
                question=query,
                retrieval_top_k=10,
                rerank_top_k=3
            )
            
            # æ˜¾ç¤ºç»“æœ
            print(f"\nğŸ’¡ ç­”æ¡ˆï¼š")
            print(result['answer'])
            
            print(f"\nğŸ“Š ç›¸å…³æ€§è¯„åˆ†ï¼š{result['confidence']:.3f}")
            
            print(f"\nğŸ“– å‚è€ƒèµ„æ–™ï¼š")
            for j, source in enumerate(result['sources'][:3], 1):
                print(f"  {j}. {source['title']} (è¯„åˆ†: {source['score']:.3f})")
            
            stats = result['pipeline_stats']
            print(f"\nâ±ï¸  æ€§èƒ½ç»Ÿè®¡ï¼š")
            print(f"  æ£€ç´¢æ–‡æ¡£æ•°ï¼š{stats['retrieval_count']}")
            print(f"  é‡æ’æ–‡æ¡£æ•°ï¼š{stats['rerank_count']}")
            print(f"  æ€»è€—æ—¶ï¼š{stats['total_time']:.2f}ç§’")
            print(f"  ç”Ÿæˆè€—æ—¶ï¼š{stats['generation_time']:.2f}ç§’")
            
            print("=" * 60)
        
        print("\nâœ… RAGæ¼”ç¤ºå®Œæˆï¼")
        print(f"\nğŸ“Š æ€»ä½“ç»Ÿè®¡ï¼š")
        print(f"  åŸå§‹æ–‡æ¡£æ•°ï¼š{len(documents)}")
        print(f"  åˆ‡ç‰‡åå—æ•°ï¼š{len(rag_pipeline.retrieval_system.documents)}")
        print(f"  BGE-m3 + BGE-reranker + LLM ä¸‰é˜¶æ®µæµæ°´çº¿è¿è¡Œæ­£å¸¸")
        print(f"  æ–‡æ¡£åˆ‡ç‰‡åŠŸèƒ½ï¼šâœ… å·²å¯ç”¨ (chunk_size=1000, overlap=200)")
        print(f"  è¯­ä¹‰æ£€ç´¢ï¼šâœ… BGE-m3å¤šè¯­è¨€åµŒå…¥")
        print(f"  ç²¾ç¡®é‡æ’ï¼šâœ… BGE-rerankeräºŒæ¬¡æ’åº") 
        print(f"  æ™ºèƒ½ç”Ÿæˆï¼šâœ… é€šä¹‰åƒé—®qwen-max")
        
    except Exception as e:
        logger.error(f"RAGç³»ç»Ÿåˆå§‹åŒ–å¤±è´¥: {e}")
        print(f"âŒ ç³»ç»Ÿé”™è¯¯: {e}")

if __name__ == "__main__":
    main() 