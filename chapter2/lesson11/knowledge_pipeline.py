#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
Knowledge Graph Construction from Text with Chain-of-Thought

This script demonstrates an advanced, robust pipeline for building a knowledge graph
from unstructured text using Large Language Models with Chain-of-Thought reasoning.

Pipeline Steps:
1.  **Candidate Extraction**: Extract entities, relations, and attributes from text
2.  **Schema Optimization**: Refine and normalize entity/relation/attribute types
3.  **Refinement & Relabeling**: Apply the optimized schema to candidates
4.  **Role Analysis**: Perform Chain-of-Thought analysis to identify unique entities
5.  **Context-Aware Entity Linking**: Use role analysis for accurate entity linking

Dependencies: pip install openai networkx pyvis
Setup: Set your LLM API key as environment variable (e.g., DASHSCOPE_API_KEY).
"""

from __future__ import annotations
import json
import os
import datetime
import logging
from pathlib import Path
from typing import Dict, List, Tuple, Any, Optional
import networkx as nx
from pyvis.network import Network
# =============================================================================
# --- PROMPT TEMPLATES SECTION  (with oneâ€‘shot demonstrations) ---
# =============================================================================

FREE_EXTRACTION_PROMPT = """-ä»»åŠ¡ç›®æ ‡-
ä½ æ˜¯ä¸€ä¸ªä¿¡æ¯æŠ½å–ä¸“å®¶ã€‚è¯·æ ¹æ®ç»™å®šæ–‡æœ¬æ„å»ºçŸ¥è¯†å›¾è°±ï¼Œè¯†åˆ«æ‰€æœ‰é‡è¦çš„å®ä½“å’Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€‚

-æ ¸å¿ƒæ„å»ºåŸåˆ™-
1.  **æ˜ç¡®åŒºåˆ†ä¸‰ç§æ¦‚å¿µç±»å‹**ï¼š
   - **å®ä½“(Entity)**: ç‹¬ç«‹å­˜åœ¨çš„å¯¹è±¡ã€äººç‰©ã€åœ°ç‚¹ã€æ¦‚å¿µç­‰ï¼ˆå¦‚ï¼šå¼ ä¸‰ã€å…¬å¸ã€åŒ—äº¬ï¼‰
   - **å…³ç³»(Relation)**: å®ä½“ä¹‹é—´çš„åŠ¨ä½œã€äº¤äº’æˆ–è”ç³»ï¼ˆå¦‚ï¼šå·¥ä½œäºã€ä½äºã€ç®¡ç†ï¼‰
   - **å±æ€§(Attribute)**: å®ä½“çš„ç‰¹å¾ã€çŠ¶æ€æˆ–æ€§è´¨ï¼ˆå¦‚ï¼šå¹´é¾„ã€é¢œè‰²ã€æƒ…æ„ŸçŠ¶æ€ï¼‰
2.  **å®ä½“è¯†åˆ«åŸåˆ™**ï¼šè¯†åˆ«å…·æœ‰ç‹¬ç«‹æ€§çš„æ ¸å¿ƒå¯¹è±¡ï¼ŒåŒ…æ‹¬ä¸“æœ‰åç§°ã€äº§å“å‹å·ã€ç»„ç»‡æœºæ„ç­‰ã€‚
3.  **å…³ç³»è¯†åˆ«åŸåˆ™**ï¼šé‡ç‚¹å…³æ³¨å®ä½“é—´çš„åŠ¨ä½œè¯ã€ä»‹è¯çŸ­è¯­å’ŒåŠ¨æ€è”ç³»ï¼Œç¡®ä¿å…³ç³»æ–¹å‘æ­£ç¡®ã€‚
4.  **å±æ€§å¤„ç†åŸåˆ™**ï¼šå°†å½¢å®¹è¯ã€çŠ¶æ€è¯ã€ç‰¹å¾è¯ä½œä¸ºå®ä½“çš„å±æ€§ï¼Œè€Œéç‹¬ç«‹å®ä½“ã€‚
5.  **å®Œæ•´æ€§åŸåˆ™**ï¼šç¡®ä¿æŠ½å–æ–‡æœ¬ä¸­æåˆ°çš„æ‰€æœ‰é‡è¦å®ä½“å’Œå…³ç³»ï¼ŒåŒ…æ‹¬æŠ€æœ¯è§„æ ¼ã€æ•°é‡ä¿¡æ¯ç­‰ã€‚
6.  **ä¼˜å…ˆä½¿ç”¨æœ€å…·ä½“çš„åç§°**ï¼ˆå¦‚ä¸“æœ‰åç§°ï¼‰ä½œä¸ºå…³ç³»çš„ä¸»è¯­å’Œå®¾è¯­ã€‚
7.  **ä¿æŒå…³ç³»çš„å…·ä½“è¯­ä¹‰**ï¼šä½¿ç”¨åŸæ–‡ä¸­çš„å…·ä½“åŠ¨ä½œè¯ï¼Œä¸è¦è¿‡åº¦æ¦‚æ‹¬ã€‚
8.  è¯†åˆ«å¹¶ç»Ÿä¸€æŒ‡å‘åŒä¸€å¯¹è±¡çš„ä¸åŒè¡¨è¿°ï¼ˆå…±æŒ‡æ¶ˆè§£ï¼‰ã€‚

-è¾“å‡ºæ ¼å¼-
è¯·ä¸¥æ ¼è¿”å›ä»¥ä¸‹JSONæ ¼å¼çš„å¯¹è±¡ï¼š
{{
  "entities": [
    {{"name": "å®ä½“åç§°", "type": "å®ä½“ç±»å‹", "description": "ç®€è¦æè¿°", "attributes": {{"å±æ€§å": "å±æ€§å€¼"}}}}
  ],
  "relations": [
    {{"subject": "ä¸»è¯­å®ä½“", "predicate": "å…³ç³»", "object": "å®¾è¯­å®ä½“"}}
  ]
}}

-ç¤ºä¾‹æ¼”ç¤º-
è¾“å…¥æ–‡æœ¬ï¼ˆã€Šä¼ä¸šç»„ç»‡æ¶æ„è¯´æ˜ã€‹èŠ‚é€‰ï¼‰ï¼š
  "èµ„æ·±å·¥ç¨‹å¸ˆææ˜è´Ÿè´£æŠ€æœ¯å›¢é˜Ÿï¼Œä»–æ€§æ ¼å¼€æœ—ä¸”ç»éªŒä¸°å¯Œã€‚ææ˜ç›´æ¥å‘CTOç‹æ€»æ±‡æŠ¥å·¥ä½œè¿›å±•ã€‚"

æœŸæœ›è¾“å‡ºï¼š
{{
  "entities": [
    {{"name":"ææ˜","type":"Person","description":"èµ„æ·±å·¥ç¨‹å¸ˆ", "attributes": {{"èŒä½": "èµ„æ·±å·¥ç¨‹å¸ˆ", "æ€§æ ¼": "å¼€æœ—", "ç»éªŒ": "ä¸°å¯Œ"}}}},
    {{"name":"æŠ€æœ¯å›¢é˜Ÿ","type":"Organization","description":"æŠ€æœ¯å¼€å‘å›¢é˜Ÿ", "attributes": {{}}}},
    {{"name":"ç‹æ€»","type":"Person","description":"CTO", "attributes": {{"èŒä½": "CTO"}}}}
  ],
  "relations":[
    {{"subject":"ææ˜","predicate":"è´Ÿè´£","object":"æŠ€æœ¯å›¢é˜Ÿ"}},
    {{"subject":"ææ˜","predicate":"æ±‡æŠ¥","object":"ç‹æ€»"}}
  ]
}}
"""

# ---------------------------------------------------------------------------

SCHEMA_OPTIMIZATION_PROMPT = """-ä»»åŠ¡ç›®æ ‡-
ä½ æ˜¯ä¸€ä¸ªæ•°æ®æ¶æ„å¸ˆã€‚è¯·åŸºäºä»¥ä¸‹ä»æ–‡æœ¬ä¸­æŠ½å–çš„çŸ¥è¯†ï¼Œä¼˜åŒ–å…¶Schemaï¼ˆå®ä½“ç±»å‹ã€å…³ç³»ç±»å‹å’Œå±æ€§ç±»å‹ï¼‰ï¼Œä½¿å…¶æ›´åŠ è§„èŒƒå’Œä¸€è‡´ã€‚

-åŸå§‹å›¾è°±ä¿¡æ¯-
å®ä½“åˆ—è¡¨ï¼ˆç¤ºä¾‹ï¼‰:
{entities}
å…³ç³»åˆ—è¡¨ï¼ˆç¤ºä¾‹ï¼‰:
{relations}

-ä¼˜åŒ–åŸåˆ™-
- åˆå¹¶ç›¸ä¼¼çš„ç±»å‹ï¼Œä½†ä¸è¦è¿‡åº¦æ³›åŒ–ã€‚
- **ä¿æŒå…³ç³»çš„å…·ä½“è¯­ä¹‰**ï¼šæ„ä¹‰æ˜ç¡®ä¸åŒçš„å…³ç³»ç±»å‹åº”ä¿æŒç‹¬ç«‹ï¼ˆå¦‚"ç®¡ç†"å’Œ"åä½œ"ã€"åˆ›å»º"å’Œ"åˆ é™¤"ç­‰ï¼‰ã€‚
- **å±æ€§ç±»å‹è§„èŒƒåŒ–**ï¼šå°†å®ä½“çš„ç‰¹å¾ã€çŠ¶æ€ã€æ€§è´¨å½’ç±»ä¸ºæ ‡å‡†å±æ€§ç±»å‹ã€‚
- ä½¿ç”¨ä¸šç•Œé€šç”¨çš„ã€ç®€æ´æ˜ç¡®çš„å‘½åã€‚
- **ä¼˜å…ˆä¿ç•™è¯­ä¹‰ä¸°å¯Œçš„åŸå§‹å…³ç³»è¯**ï¼Œé™¤éç¡®å®éœ€è¦è§„èŒƒåŒ–ã€‚

-è¾“å‡ºæ ¼å¼-
è¯·è¿”å›ä¸€ä¸ªåŒ…å«ä¼˜åŒ–åç±»å‹åˆ—è¡¨çš„JSONå¯¹è±¡ï¼š
{{
  "entities": ["ä¼˜åŒ–åçš„å®ä½“ç±»å‹1", ...],
  "relations": ["ä¼˜åŒ–åçš„å…³ç³»ç±»å‹1", ...],
  "attributes": ["ä¼˜åŒ–åçš„å±æ€§ç±»å‹1", ...]
}}

-ç¤ºä¾‹æ¼”ç¤º-
è¾“å…¥ï¼ˆèŠ‚é€‰ï¼‰ï¼š
  å®ä½“ç±»å‹: ["Person","å‘˜å·¥","Manager","ç³»ç»Ÿ","Module"]
  å…³ç³»ç±»å‹: ["ç®¡ç†","supervise","è´Ÿè´£","åˆ›å»º","ç”Ÿæˆ"]
  å±æ€§ç±»å‹: ["å¹´é¾„","å§“å","çŠ¶æ€","çº§åˆ«","ç»éªŒ"]

ç¤ºä¾‹è¾“å‡ºï¼š
{{
  "entities": ["Person","System"],
  "relations": ["ç®¡ç†","åˆ›å»º"],
  "attributes": ["åŸºæœ¬ä¿¡æ¯","çŠ¶æ€","çº§åˆ«"]
}}
"""

# ---------------------------------------------------------------------------

REFINE_AND_RELABEL_PROMPT = """-ä»»åŠ¡ç›®æ ‡-
ä½ æ˜¯ä¸€ä¸ªçŸ¥è¯†å·¥ç¨‹å¸ˆã€‚ä½ çš„ä»»åŠ¡æ˜¯æ ¹æ®ä¸€ä¸ª"ä¼˜åŒ–åçš„æ ‡å‡†Schema"ï¼Œå¯¹ä¸€ä»½"å€™é€‰ä¸‰å…ƒç»„åˆ—è¡¨"è¿›è¡Œç²¾ç‚¼ã€é‡æ ‡è®°å’Œè¿‡æ»¤ã€‚

-ä¼˜åŒ–åçš„æ ‡å‡†Schema-
è¿™æ˜¯å”¯ä¸€å…è®¸ä½¿ç”¨çš„ç±»å‹å®šä¹‰ï¼š
{schema}

-å€™é€‰ä¸‰å…ƒç»„åˆ—è¡¨-
è¿™æ˜¯éœ€è¦è¢«å¤„ç†çš„æ•°æ®ï¼š
{triples}

-å¤„ç†æŒ‡ä»¤-
1.  éå†æ¯ä¸€æ¡å€™é€‰ä¸‰å…ƒç»„ã€‚
2.  å¯¹äºä¸‰å…ƒç»„ä¸­çš„è°“è¯(predicate)ï¼Œå°†å…¶é‡å‘½åä¸ºæ–°Schemaä¸­æœ€åŒ¹é…çš„å…³ç³»ç±»å‹ã€‚
3.  **ä¼˜å…ˆä¿ç•™å…³ç³»**ï¼šå°½é‡å°†åŸå§‹å…³ç³»æ˜ å°„åˆ°Schemaä¸­è¯­ä¹‰æœ€æ¥è¿‘çš„å…³ç³»ç±»å‹ï¼Œåªæœ‰åœ¨å®Œå…¨æ— æ³•æ˜ å°„æ—¶æ‰ä¸¢å¼ƒã€‚

-è¾“å‡ºæ ¼å¼-
è¯·è¿”å›ä¸€ä¸ªåªåŒ…å«ç²¾ç‚¼åä¸‰å…ƒç»„åˆ—è¡¨çš„JSONå¯¹è±¡ï¼š
{{
  "refined_triples": [
    {{"subject": "ä¸»è¯­å®ä½“", "predicate": "ç¬¦åˆæ–°Schemaçš„å…³ç³»", "object": "å®¾è¯­å®ä½“"}}
  ]
}}

-ç¤ºä¾‹æ¼”ç¤º-
æ ‡å‡†Schemaï¼ˆç¤ºä¾‹ï¼‰ï¼š
  å®ä½“ç±»å‹: ["Actor","Module"]
  å…³ç³»ç±»å‹: ["åˆ›å»º","è§¦å‘"]

å€™é€‰ä¸‰å…ƒç»„ï¼š
  [
    {{"subject":"ç³»ç»Ÿ","predicate":"generate","object":"æ—¥å¿—æ¨¡å—"}},
    {{"subject":"ç”¨æˆ·","predicate":"produce","object":"è¯·æ±‚"}}
  ]

ç¤ºä¾‹è¾“å‡ºï¼š
{{
  "refined_triples": [
    {{"subject":"ç³»ç»Ÿ","predicate":"åˆ›å»º","object":"æ—¥å¿—æ¨¡å—"}}
  ]
}}
"""

# ---------------------------------------------------------------------------

ROLE_ANALYSIS_PROMPT = """-ä»»åŠ¡ç›®æ ‡-
ä½ æ˜¯ä¸€ä½é€»è¾‘ç¼œå¯†çš„æ–‡æ¡£åˆ†æä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»”ç»†é˜…è¯»ä»¥ä¸‹æ–‡æœ¬ï¼Œå¹¶ç¡®å®šæ–‡æ¡£ä¸­æ¯ç§â€œè§’è‰²ç±»å‹â€ï¼ˆå¦‚ç³»ç»Ÿç®¡ç†å‘˜ã€è®¿å®¢ï¼‰å„æœ‰å¤šå°‘ä¸ªç‹¬ç«‹çš„ä¸ªä½“ã€‚

-å¾…åˆ†æçš„æ–‡æœ¬-
{text}

-å¤„ç†æŒ‡ä»¤-
1.  é€šè¯»å…¨æ–‡ï¼Œç†è§£å…¶å†…å®¹ä¸åœºæ™¯ã€‚
2.  è¯†åˆ«å‡ºç°çš„è§’è‰²ç±»å‹ï¼ˆä¾‹å¦‚ï¼šç³»ç»Ÿç®¡ç†å‘˜ã€è®¿å®¢ã€å®¡è®¡å‘˜ç­‰ï¼‰ã€‚
3.  **å…³é”®æ­¥éª¤ - åŒä¸€å®ä½“è¯†åˆ«**ï¼šä»”ç»†åˆ†ææ¯ä¸ªè§’è‰²ç±»å‹ä¸‹çš„æ‰€æœ‰æè¿°ï¼Œåˆ¤æ–­å“ªäº›æè¿°æŒ‡å‘åŒä¸€ä¸ªä½“ï¼š
   - å¯»æ‰¾ä¸“æœ‰åç§°ï¼ˆå¦‚äººåã€ç¼–å·ï¼‰ä½œä¸ºå”¯ä¸€æ ‡è¯†ç¬¦
   - åˆ†æä¸Šä¸‹æ–‡è¿è´¯æ€§ï¼Œè¯†åˆ«åŒä¸€ä¸ªä½“çš„ä¸åŒæè¿°æ–¹å¼
   - æ³¨æ„ä¿®é¥°è¯å’Œå±æ€§æè¿°é€šå¸¸ä¸ä¼šåˆ›é€ æ–°ä¸ªä½“ï¼Œè€Œæ˜¯å¯¹ç°æœ‰ä¸ªä½“çš„è¿›ä¸€æ­¥æè¿°
4.  å¯¹äºæ¯ç§ç±»å‹ï¼ŒåŸºäºæ­¥éª¤3çš„åˆ†æï¼Œç¡®å®šçœŸæ­£ç‹¬ç«‹çš„ä¸ªä½“æ•°é‡ã€‚
5.  åœ¨reasoningå­—æ®µä¸­ï¼Œè¯¦ç»†è¯´æ˜ä½ çš„åˆ¤æ–­ä¾æ®ï¼Œç‰¹åˆ«æ˜¯å¦‚ä½•åŒºåˆ†åŒä¸€ä¸ªä½“çš„ä¸åŒæè¿°ã€‚

-è¾“å‡ºæ ¼å¼-
è¯·è¿”å›ä¸€ä¸ªåŒ…å«ä½ çš„åˆ†æå’Œç»“è®ºçš„JSONå¯¹è±¡ï¼š
{{
  "reasoning": "ä½ çš„åˆ¤æ–­é€»è¾‘",
  "character_counts": {{
    "è§’è‰²ç±»å‹1": æ•°é‡1,
    "è§’è‰²ç±»å‹2": æ•°é‡2
  }}
}}

-ç¤ºä¾‹æ¼”ç¤º-
è¾“å…¥æ–‡æœ¬ï¼ˆã€Šç³»ç»Ÿåˆ†æè¯´æ˜ä¹¦ã€‹èŠ‚é€‰ï¼‰ï¼š
  â€œæœ¬ç³»ç»Ÿæ¶‰åŠä¸‰ç±»ç”¨æˆ·ï¼šç³»ç»Ÿç®¡ç†å‘˜ï¼ˆAdminâ€‘Aã€Adminâ€‘Bï¼‰ã€æ™®é€šç”¨æˆ·ï¼ˆUserâ€‘001 è‡³ Userâ€‘050ï¼‰ï¼Œä»¥åŠå®¡è®¡å‘˜ï¼ˆAuditorâ€‘Zï¼‰ã€‚ç®¡ç†å‘˜è´Ÿè´£é…ç½®ç³»ç»Ÿï¼›æ™®é€šç”¨æˆ·è¿›è¡Œæ—¥å¸¸æ“ä½œï¼›å®¡è®¡å‘˜å®šæœŸå®¡æŸ¥æ—¥å¿—ã€‚â€

ç¤ºä¾‹è¾“å‡ºï¼š
{{
  "reasoning": "æ–‡æ¡£æ˜ç¡®åˆ—å‡º Adminâ€‘A å’Œ Adminâ€‘B ä¸¤åç®¡ç†å‘˜ï¼ŒUserâ€‘001 è‡³ Userâ€‘050 å…± 50 åæ™®é€šç”¨æˆ·ï¼Œä»¥åŠ 1 åå®¡è®¡å‘˜ã€‚",
  "character_counts": {{
    "ç³»ç»Ÿç®¡ç†å‘˜": 2,
    "æ™®é€šç”¨æˆ·": 50,
    "å®¡è®¡å‘˜": 1
  }}
}}
"""

# ---------------------------------------------------------------------------

CONTEXT_AWARE_ENTITY_LINKING_PROMPT = """-ä»»åŠ¡ç›®æ ‡-
ä½ æ˜¯ä¸€ä¸ªå®ä½“é“¾æ¥ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯åˆ†æä¸€ä¸ªå®ä½“åç§°åˆ—è¡¨ï¼Œå¹¶å°†æŒ‡å‘åŒä¸€ä¸ªçœŸå®ä¸–ç•Œç‰©ä½“çš„ä¸åŒåç§°ï¼ˆåˆ«åï¼‰é“¾æ¥åˆ°ä¸€ä¸ªç»Ÿä¸€çš„"æ ‡å‡†å"ã€‚

-é‡è¦ä¸Šä¸‹æ–‡ï¼ˆé¢„åˆ†æç»“è®ºï¼‰-
æ ¹æ®å¯¹æ–‡æ¡£çš„é¢„å…ˆåˆ†æï¼Œæˆ‘ä»¬ç¡®å®šè§’è‰²çš„æ•°é‡å¦‚ä¸‹ã€‚è¯·å°†æ­¤ä½œä¸ºä½ åˆ¤æ–­çš„"é»„é‡‘å‡†åˆ™"ï¼š
{character_analysis}

-å¾…é“¾æ¥çš„å®ä½“åˆ—è¡¨-
{entities}

-å¤„ç†æŒ‡ä»¤-
1.  **ä¸¥æ ¼éµå®ˆä¸Šè¿°ä¸Šä¸‹æ–‡ç»“è®º**ã€‚æ ¹æ®è§’è‰²ç±»å‹çš„æ•°é‡é™åˆ¶ï¼Œå°†æ‰€æœ‰æŒ‡å‘åŒä¸€ç±»å‹åŒä¸€ä¸ªä½“çš„ä¸åŒæè¿°é“¾æ¥åˆ°åŒä¸€ä¸ªæ ‡å‡†åã€‚
2.  **ç³»ç»Ÿæ€§åˆ†ææ­¥éª¤**ï¼š
   - é¦–å…ˆè¯†åˆ«ä¸“æœ‰åç§°ï¼ˆäººåã€ç¼–å·ç­‰ï¼‰ä½œä¸ºå®ä½“çš„æ ¸å¿ƒæ ‡è¯†
   - ç„¶ååˆ†ææè¿°æ€§è¯æ±‡ï¼ˆå½¢å®¹è¯ã€é€šç”¨åè¯ï¼‰æ˜¯å¦æŒ‡å‘å·²è¯†åˆ«çš„ä¸“æœ‰åç§°å®ä½“
   - æ£€æŸ¥è¯­ä¹‰ç›¸å…³æ€§å’Œä¸Šä¸‹æ–‡ä¸€è‡´æ€§
   - è€ƒè™‘ä¿®é¥°è¯å±‚æ¬¡å…³ç³»ï¼ˆå¦‚"é«˜ä¸ªå­å­¦ç”Ÿ"å¯èƒ½æ˜¯å¯¹æŸä¸ªå…·åå­¦ç”Ÿçš„æè¿°ï¼‰
3.  ä¸ºæ¯ä¸€ç»„åˆ«åï¼Œé€‰æ‹©ä¸€ä¸ªæœ€å®Œæ•´ã€æœ€æ˜ç¡®çš„åç§°ä½œä¸ºè¯¥ç»„çš„"æ ‡å‡†å"ã€‚**å¦‚æœå­˜åœ¨ä¸“æœ‰åç§°ï¼ˆå¦‚äººåã€åœ°åï¼‰ï¼Œä¼˜å…ˆé€‰æ‹©å®ƒä½œä¸ºæ ‡å‡†åã€‚**
4.  **éªŒè¯æ­¥éª¤**ï¼šç¡®ä¿é“¾æ¥ç»“æœç¬¦åˆä¸Šä¸‹æ–‡ç»™å‡ºçš„æ•°é‡é™åˆ¶ã€‚
5.  æœ€ç»ˆè¾“å‡ºå¿…é¡»æ˜¯ä¸€ä¸ªJSONå¯¹è±¡ï¼Œå…¶ä¸­æ¯ä¸ªè¾“å…¥å®ä½“éƒ½ä½œä¸ºkeyï¼Œå…¶å¯¹åº”çš„"æ ‡å‡†å"ä½œä¸ºvalueã€‚

-è¾“å‡ºæ ¼å¼-
è¯·è¿”å›ä¸€ä¸ªJSONå¯¹è±¡ï¼Œæ ¼å¼ä¸º {{ "åˆ«å": "æ ‡å‡†å", ... }}:
{{
  "alias_map": {{
    "åˆ«å1": "æ ‡å‡†åA",
    "åˆ«å2": "æ ‡å‡†åA",
    ...
  }}
}}

-ç¤ºä¾‹æ¼”ç¤º-
ä¸Šä¸‹æ–‡ï¼ˆç¤ºä¾‹ï¼‰ï¼š
  {{
    "character_counts": {{
      "ç³»ç»Ÿç®¡ç†å‘˜": 2,
      "æ™®é€šç”¨æˆ·": 3
    }}
  }}

å¾…é“¾æ¥å®ä½“ï¼š
  ["Adminâ€‘A","ç®¡ç†å‘˜A","ç³»ç»Ÿç®¡ç†å‘˜A","Adminâ€‘B","ç®¡ç†å‘˜B","Userâ€‘001","Uâ€‘1","Userâ€‘002","Userâ€‘003"]

ç¤ºä¾‹è¾“å‡ºï¼š
{{
  "alias_map": {{
    "Adminâ€‘A": "Adminâ€‘A",
    "ç®¡ç†å‘˜A": "Adminâ€‘A",
    "ç³»ç»Ÿç®¡ç†å‘˜A": "Adminâ€‘A",
    "Adminâ€‘B": "Adminâ€‘B",
    "ç®¡ç†å‘˜B": "Adminâ€‘B",
    "Userâ€‘001": "Userâ€‘001",
    "Uâ€‘1": "Userâ€‘001",
    "Userâ€‘002": "Userâ€‘002",
    "Userâ€‘003": "Userâ€‘003"
  }}
}}
"""


# =============================================================================
# --- CONFIGURATION AND CLIENT SETUP (UNCHANGED) ---
# =============================================================================
LLM_MODEL = os.environ.get("LLM_MODEL", "qwen-plus")
DASHSCOPE_API_KEY = os.environ.get("DASHSCOPE_API_KEY")
if not DASHSCOPE_API_KEY:
    raise ValueError("è¯·è®¾ç½®ç¯å¢ƒå˜é‡ DASHSCOPE_API_KEYã€‚åœ¨ ~/.bashrc ä¸­æ·»åŠ ï¼šexport DASHSCOPE_API_KEY='your_api_key'")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
DEFAULT_DATA_DIR = Path("examples/demo_outputs")

try:
    from openai import OpenAI
except ImportError:
    OpenAI = None

def get_llm_client() -> Any:
    # (This function is unchanged from the previous version)
    if OpenAI is None:
        raise RuntimeError("The 'openai' Python SDK is not installed. Please run 'pip install openai'.")
    provider = "dashscope" if "qwen" in LLM_MODEL.lower() else "openai"
    api_key, base_url = None, None
    if provider == "dashscope":
        api_key, base_url = DASHSCOPE_API_KEY, "https://dashscope.aliyuncs.com/compatible-mode/v1"
        if not api_key: raise ValueError("DASHSCOPE_API_KEY environment variable not set.")
    else:
        api_key = OPENAI_API_KEY
        if not api_key: raise ValueError("OPENAI_API_KEY environment variable not set.")
    print(f"âœ… Using {provider.title()} API with model: {LLM_MODEL}")
    return OpenAI(api_key=api_key, base_url=base_url)

# =============================================================================
# --- CORE LOGIC CLASSES (UPDATED) ---
# =============================================================================
class InformationExtractor:
    """Encapsulates LLM calls, now including a role analysis step."""
    def __init__(self, client: Any, log_dir: Optional[Path] = None):
        self.client = client
        self.model = LLM_MODEL
        self.log_dir = log_dir or DEFAULT_DATA_DIR
        self.log_dir.mkdir(parents=True, exist_ok=True)
        self.timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # Setup unified logging
        self.logger = self._setup_logger()

    def _setup_logger(self) -> logging.Logger:
        """Setup unified logger for the pipeline."""
        logger = logging.getLogger(f"KnowledgeGraph_{self.timestamp}")
        logger.setLevel(logging.INFO)
        
        # Remove existing handlers
        for handler in logger.handlers[:]:
            logger.removeHandler(handler)
        
        # Create file handler
        log_file = self.log_dir / f"pipeline_log_{self.timestamp}.log"
        file_handler = logging.FileHandler(log_file, encoding='utf-8')
        file_handler.setLevel(logging.INFO)
        
        # Create formatter
        formatter = logging.Formatter(
            '%(asctime)s - %(levelname)s - %(message)s',
            datefmt='%Y-%m-%d %H:%M:%S'
        )
        file_handler.setFormatter(formatter)
        
        logger.addHandler(file_handler)
        print(f"ğŸ“‹ Unified pipeline log: {log_file}")
        return logger

    def _log_step(self, step_name: str, data: Dict[str, Any]) -> None:
        """Log step information in a structured format."""
        self.logger.info(f"\n{'='*60}")
        self.logger.info(f"STEP: {step_name}")
        self.logger.info(f"{'='*60}")
        
        for key, value in data.items():
            if isinstance(value, (dict, list)):
                self.logger.info(f"{key}:\n{json.dumps(value, ensure_ascii=False, indent=2)}")
            else:
                self.logger.info(f"{key}: {value}")

    def _call_llm(self, prompt: str, task_name: str) -> Dict[str, Any]:
        print(f"    ğŸ¤– Sending request to LLM for: {task_name}...")
        
        # Log the request
        self.logger.info(f"\n--- LLM REQUEST: {task_name} ---")
        self.logger.info(f"Prompt:\n{prompt}")
        
        try:
            resp = self.client.chat.completions.create(
                model=self.model, messages=[{"role": "user", "content": prompt}],
                temperature=0.0, response_format={"type": "json_object"},
            )
            content = resp.choices[0].message.content
            print(f"    âœ… LLM response received for {task_name}.")
            
            # Log the response
            self.logger.info(f"Response:\n{content}")
            self.logger.info(f"--- END LLM REQUEST: {task_name} ---\n")
            
            return json.loads(content) if content else {}
        except Exception as e:
            print(f"    âŒ [Error] LLM call for {task_name} failed: {e}")
            self.logger.error(f"LLM call failed for {task_name}: {str(e)}")
            return {}

    def extract_candidates(self, text: str) -> Tuple[Dict, List]:
        # (Unchanged)
        full_prompt = f"{FREE_EXTRACTION_PROMPT}\n\næ–‡æœ¬:\n---\n{text}\n---"
        response_data = self._call_llm(full_prompt, "Candidate Extraction")
        entities, triples = self._parse_extraction_output(response_data)
        
        # Save intermediate results
        candidates_data = {
            "input_text": text,
            "extracted_entities": entities,
            "extracted_triples": triples,
            "entity_count": len(entities),
            "triple_count": len(triples)
        }
        self._log_step("candidates_extraction", candidates_data)
        
        return entities, triples

    def optimize_ontology(self, entities: Dict, relations: List) -> Dict:
        # (Unchanged)
        entities_info = [f"- {name} (ç±»å‹: {data.get('type', 'N/A')})" for name, data in entities.items()]
        relations_info = [f"- {rel.get('subject')} --{rel.get('predicate')}--> {rel.get('object')}" for rel in relations]
        prompt = SCHEMA_OPTIMIZATION_PROMPT.format(entities='\n'.join(entities_info), relations='\n'.join(relations_info))
        response_data = self._call_llm(prompt, "Schema Optimization")
        
        # Save intermediate results
        optimization_data = {
            "input_entities": entities,
            "input_relations": relations,
            "optimized_schema": response_data,
            "entity_types_optimized": response_data.get("entities", []),
            "relation_types_optimized": response_data.get("relations", []),
            "attribute_types_optimized": response_data.get("attributes", [])
        }
        self._log_step("schema_optimization", optimization_data)
        
        return response_data

    def refine_and_relabel(self, triples: List, schema: Dict) -> List:
        # (Unchanged)
        prompt = REFINE_AND_RELABEL_PROMPT.format(
            triples=json.dumps(triples, ensure_ascii=False, indent=2),
            schema=json.dumps(schema, ensure_ascii=False, indent=2)
        )
        response_data = self._call_llm(prompt, "Refinement and Relabeling")
        refined_triples = response_data.get("refined_triples", [])
        valid_triples = [t for t in refined_triples if isinstance(t, dict) and all(k in t for k in ['subject', 'predicate', 'object'])]
        
        # Save intermediate results
        refinement_data = {
            "input_triples": triples,
            "input_schema": schema,
            "raw_refined_triples": refined_triples,
            "valid_refined_triples": valid_triples,
            "input_count": len(triples),
            "output_count": len(valid_triples),
            "filtered_count": len(refined_triples) - len(valid_triples)
        }
        self._log_step("refinement_and_relabeling", refinement_data)
        
        return valid_triples

    def analyze_roles(self, text: str) -> Dict:
        """NEW METHOD: Performs high-level role analysis."""
        prompt = ROLE_ANALYSIS_PROMPT.format(text=text)
        response_data = self._call_llm(prompt, "Role Analysis")
        
        # Save intermediate results
        role_analysis_data = {
            "input_text": text,
            "analysis_result": response_data,
            "reasoning": response_data.get("reasoning", ""),
            "character_counts": response_data.get("character_counts", {})
        }
        self._log_step("role_analysis", role_analysis_data)
        
        return response_data

    def link_entities(self, entity_names: List[str], character_analysis: Dict) -> Dict[str, str]:
        """UPDATED METHOD: Now uses context for linking."""
        prompt = CONTEXT_AWARE_ENTITY_LINKING_PROMPT.format(
            entities=json.dumps(entity_names, ensure_ascii=False),
            character_analysis=json.dumps(character_analysis, ensure_ascii=False, indent=2)
        )
        response_data = self._call_llm(prompt, "Context-Aware Entity Linking")
        alias_map = response_data.get("alias_map", {})
        
        # Save intermediate results
        linking_data = {
            "input_entity_names": entity_names,
            "character_analysis_context": character_analysis,
            "raw_response": response_data,
            "alias_map": alias_map,
            "linking_pairs": len([k for k, v in alias_map.items() if k != v])
        }
        self._log_step("entity_linking", linking_data)
        
        return alias_map

    def _parse_extraction_output(self, data: Dict) -> Tuple[Dict, List]:
        """è§£æLLMè¾“å‡ºï¼Œå¤„ç†å®ä½“ï¼ˆåŒ…å«å±æ€§ï¼‰å’Œå…³ç³»"""
        if not isinstance(data, dict): return {}, []
        
        raw_entities = data.get("entities", [])
        raw_relations = data.get("relations", [])
        
        # å¤„ç†å®ä½“ï¼ˆåŒ…æ‹¬å±æ€§ï¼‰
        entities = {}
        for e in raw_entities:
            if isinstance(e, dict) and 'name' in e:
                entity_name = e['name']
                entity_data = {
                    'name': entity_name,
                    'type': e.get('type', 'æœªçŸ¥'),
                    'description': e.get('description', ''),
                    'attributes': e.get('attributes', {})
                }
                entities[entity_name] = entity_data
        
        # å¤„ç†å…³ç³»ï¼ˆåªä¿ç•™å®ä½“é—´çš„å…³ç³»ï¼‰
        triples = []
        for r in raw_relations:
            if isinstance(r, dict) and all(k in r for k in ['subject', 'predicate', 'object']):
                triples.append(r)
        
        print(f"    ğŸ“Š Parsed: {len(entities)} entities, {len(triples)} triples")
        return entities, triples

class KnowledgeGraph:
    # (This class is unchanged from the previous version)
    def __init__(self):
        self.graph = nx.MultiDiGraph()  # æ”¯æŒå¤šé‡è¾¹ï¼Œé˜²æ­¢å…³ç³»è¦†ç›–

    def build_from_entities_and_triples(self, entities: Dict, triples: List[Dict]):
        """ä»å®ä½“ï¼ˆåŒ…å«å±æ€§ï¼‰å’Œä¸‰å…ƒç»„æ„å»ºå›¾è°±"""
        # å…ˆæ·»åŠ æ‰€æœ‰å®ä½“èŠ‚ç‚¹ï¼ˆåŒ…å«å±æ€§ä¿¡æ¯ï¼‰
        for entity_name, entity_data in entities.items():
            attributes = entity_data.get('attributes', {})
            self.graph.add_node(
                entity_name, 
                type=entity_data.get('type', 'æœªçŸ¥'),
                description=entity_data.get('description', ''),
                **attributes  # å°†å±æ€§ä½œä¸ºèŠ‚ç‚¹å±æ€§å­˜å‚¨
            )
        
        # ç„¶åæ·»åŠ å…³ç³»è¾¹
        for triple in triples:
            sub, pred, obj = triple.get('subject'), triple.get('predicate'), triple.get('object')
            if not all([sub, pred, obj]): 
                continue
            
            # é€šç”¨è¿‡æ»¤ï¼šå»é™¤æ— æ„ä¹‰çš„å…³ç³»
            if self._should_filter_relation(str(sub), str(pred), str(obj)):
                continue
            
            # ç¡®ä¿ä¸»è¯­å’Œå®¾è¯­éƒ½æ˜¯å®ä½“ï¼ˆè€Œéå±æ€§å€¼ï¼‰
            if sub in entities and obj in entities:
                self.graph.add_edge(sub, obj, predicate=pred)

    def build_from_triples(self, triples: List[Dict]):
        """å…¼å®¹æ€§æ–¹æ³•ï¼šä»…ä»ä¸‰å…ƒç»„æ„å»ºå›¾è°±"""
        for triple in triples:
            sub, pred, obj = triple.get('subject'), triple.get('predicate'), triple.get('object')
            if not all([sub, pred, obj]): 
                continue
            
            # é€šç”¨è¿‡æ»¤ï¼šå»é™¤æ— æ„ä¹‰çš„å…³ç³»
            if self._should_filter_relation(str(sub), str(pred), str(obj)):
                continue
                
            self.graph.add_node(sub, type='æœªçŸ¥')
            self.graph.add_node(obj, type='æœªçŸ¥')
            self.graph.add_edge(sub, obj, predicate=pred)
    
    def _should_filter_relation(self, subject: str, predicate: str, object: str) -> bool:
        """é€šç”¨å…³ç³»è¿‡æ»¤å™¨ï¼Œå»é™¤æ— æ„ä¹‰çš„å…³ç³»"""
        # è¿‡æ»¤è‡ªç¯å…³ç³»ï¼ˆä¸»è¯­å’Œå®¾è¯­ç›¸åŒï¼‰
        if subject == object:
            return True
        
        # è¿‡æ»¤æ— æ„ä¹‰çš„"æ˜¯"å…³ç³»ï¼ˆé€šå¸¸è¡¨ç¤ºç±»å‹å½’å±ï¼Œä¸æ˜¯å®ä½“é—´å…³ç³»ï¼‰
        if predicate.lower() in ['æ˜¯', 'is', 'be', 'å±äº', 'belong']:
            return True
            
        # è¿‡æ»¤ç©ºå…³ç³»æˆ–è¿‡çŸ­å…³ç³»
        if len(predicate.strip()) <= 1:
            return True
            
        return False

    def apply_entity_mapping(self, mapping: Dict[str, str]):
        nodes_to_remove = {alias for alias, canonical in mapping.items() if alias != canonical and alias in self.graph}
        nx.relabel_nodes(self.graph, mapping, copy=False)
        print(f"    ğŸ”— Merged {len(nodes_to_remove)} alias nodes into their canonical forms.")

    def assess_quality(self) -> Dict[str, Any]:
        """Enhanced quality assessment with detailed diagnostics."""
        assessment = {
            "total_nodes": self.graph.number_of_nodes(),
            "total_edges": self.graph.number_of_edges(),
            "issues": [],
            "warnings": [],
            "stats": {}
        }
        
        if assessment["total_nodes"] == 0:
            assessment["issues"].append("å›¾è°±ä¸ºç©ºï¼Œæ²¡æœ‰ä»»ä½•èŠ‚ç‚¹")
            return assessment
            
        # 1. Check isolated nodes
        isolated_nodes = list(nx.isolates(self.graph))
        if isolated_nodes:
            assessment["issues"].append(f"å‘ç° {len(isolated_nodes)} ä¸ªå­¤ç«‹èŠ‚ç‚¹ï¼ˆæ— è¿æ¥ï¼‰")
            assessment["stats"]["isolated_nodes"] = len(isolated_nodes)
        
        # 2. Check self-loops
        self_loops = list(nx.selfloop_edges(self.graph))
        if self_loops:
            assessment["warnings"].append(f"å‘ç° {len(self_loops)} ä¸ªè‡ªç¯å…³ç³»")
            assessment["stats"]["self_loops"] = len(self_loops)
        
        # 3. Entity type distribution
        node_types = nx.get_node_attributes(self.graph, "type")
        type_counts = {}
        for node_type in node_types.values():
            type_counts[node_type] = type_counts.get(node_type, 0) + 1
        assessment["stats"]["entity_type_distribution"] = type_counts
        
        # 4. Relation type distribution
        edge_predicates = nx.get_edge_attributes(self.graph, "predicate")
        predicate_counts = {}
        for predicate in edge_predicates.values():
            predicate_counts[predicate] = predicate_counts.get(predicate, 0) + 1
        assessment["stats"]["relation_type_distribution"] = predicate_counts
        
        # 5. Check connectivity
        if not nx.is_weakly_connected(self.graph):
            components = list(nx.weakly_connected_components(self.graph))
            assessment["warnings"].append(f"å›¾è°±ä¸å®Œå…¨è¿é€šï¼Œæœ‰ {len(components)} ä¸ªè¿é€šåˆ†é‡")
            assessment["stats"]["connected_components"] = len(components)
        
        # 6. Calculate density
        if assessment["total_nodes"] > 1:
            density = nx.density(self.graph)
            assessment["stats"]["graph_density"] = round(density, 4)
            if density < 0.1:
                assessment["warnings"].append("å›¾è°±å¯†åº¦è¾ƒä½ï¼Œå¯èƒ½å­˜åœ¨è¿æ¥ä¸è¶³çš„é—®é¢˜")
        
        return assessment

    def print_quality_report(self, title: str):
        """Print enhanced quality assessment report."""
        assessment = self.assess_quality()
        
        print(f"\n  ğŸ“Š {title} è´¨é‡è¯„ä¼°æŠ¥å‘Š:")
        print(f"    ğŸ“ˆ åŸºæœ¬ç»Ÿè®¡: {assessment['total_nodes']} ä¸ªèŠ‚ç‚¹, {assessment['total_edges']} æ¡è¾¹")
        
        if assessment.get("stats", {}).get("graph_density"):
            print(f"    ğŸ”— å›¾è°±å¯†åº¦: {assessment['stats']['graph_density']}")
        
        if assessment.get("stats", {}).get("entity_type_distribution"):
            print(f"    ğŸ“‹ å®ä½“ç±»å‹åˆ†å¸ƒ:")
            for etype, count in assessment["stats"]["entity_type_distribution"].items():
                print(f"      - {etype}: {count}")
        
        if assessment.get("stats", {}).get("relation_type_distribution"):
            print(f"    ğŸ”— å…³ç³»ç±»å‹åˆ†å¸ƒ:")
            for rtype, count in assessment["stats"]["relation_type_distribution"].items():
                print(f"      - {rtype}: {count}")
        
        if assessment["issues"]:
            print(f"    âŒ å‘ç°é—®é¢˜:")
            for issue in assessment["issues"]:
                print(f"      - {issue}")
        
        if assessment["warnings"]:
            print(f"    âš ï¸ æ³¨æ„äº‹é¡¹:")
            for warning in assessment["warnings"]:
                print(f"      - {warning}")
        
        if not assessment["issues"] and not assessment["warnings"]:
            print(f"    âœ… è´¨é‡æ£€æŸ¥é€šè¿‡ï¼Œæœªå‘ç°æ˜æ˜¾é—®é¢˜")

    def visualize(self, title: str, save_path: Path):
        """Enhanced interactive visualization using Pyvis."""
        if self.graph.number_of_nodes() == 0:
            print("  âš ï¸ Graph is empty, skipping visualization.")
            return

        # Create a pyvis Network object with enhanced settings
        net = Network(
            height="600px", 
            width="100%", 
            bgcolor="#222222", 
            directed=True
        )
        
        # Get node types for color grouping
        type_attr = nx.get_node_attributes(self.graph, "type")
        unique_types = sorted(list(set(type_attr.values())))
        
        # Enhanced color scheme for different entity types
        colors = ["#FF6B6B", "#4ECDC4", "#45B7D1", "#96CEB4", "#FECA57", "#FF9FF3", "#54A0FF", "#5F27CD"]
        type_color_map = {etype: colors[i % len(colors)] for i, etype in enumerate(unique_types)}
        
        # Add nodes with enhanced styling
        for node in self.graph.nodes():
            node_type = type_attr.get(node, "æœªçŸ¥")
            color = type_color_map.get(node_type, "#CCCCCC")
            
            # è·å–èŠ‚ç‚¹çš„æ‰€æœ‰å±æ€§
            node_data = self.graph.nodes[node]
            attributes_info = []
            for key, value in node_data.items():
                if key not in ['type', 'description']:  # æ’é™¤åŸºæœ¬å­—æ®µ
                    attributes_info.append(f"{key}: {value}")
            
            # Create enhanced hover information
            hover_parts = [f"èŠ‚ç‚¹: {node}", f"ç±»å‹: {node_type}"]
            if node_data.get('description'):
                hover_parts.append(f"æè¿°: {node_data['description']}")
            if attributes_info:
                hover_parts.append("å±æ€§:")
                hover_parts.extend([f"  â€¢ {attr}" for attr in attributes_info])
            
            hover_info = "\\n".join(hover_parts)
            
            net.add_node(
                node, 
                label=str(node),
                title=hover_info,
                color=color,
                size=25,
                font={'size': 14, 'color': 'white', 'bold': True}
            )
        
        # Add edges with relationship labels
        for u, v, data in self.graph.edges(data=True):
            predicate = data.get('predicate', 'å…³ç³»')
            hover_info = f"{u} --[{predicate}]--> {v}"
            
            net.add_edge(
                u, v,
                title=hover_info,
                label=predicate,
                color={'color': '#AAAAAA', 'highlight': '#FF6B6B'},
                width=2,
                arrows={'to': {'enabled': True, 'scaleFactor': 1.2}},
                font={'size': 10, 'color': 'yellow', 'strokeWidth': 2, 'strokeColor': 'black'}
            )
        
        # Configure physics and layout
        net.set_options("""
        var options = {
            "physics": {
                "enabled": true,
                "stabilization": {
                    "enabled": true,
                    "iterations": 100
                },
                "barnesHut": {
                    "gravitationalConstant": -8000,
                    "centralGravity": 0.3,
                    "springLength": 95,
                    "springConstant": 0.04,
                    "damping": 0.09
                }
            },
            "interaction": {
                "hover": true,
                "selectConnectedEdges": true,
                "tooltipDelay": 300
            },
            "layout": {
                "improvedLayout": true
            }
        }
        """)
        
        # Change file extension to .html
        html_path = save_path.with_suffix('.html')
        
        # Save the visualization
        net.save_graph(str(html_path))
        print(f"  ğŸ¨ Interactive visualization saved to: {html_path}")

    def save(self, path: Path) -> None:
        """Save knowledge graph to JSON file."""
        path.parent.mkdir(parents=True, exist_ok=True)
        data = nx.node_link_data(self.graph)
        with path.open("w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        print(f"  ğŸ’¾ Knowledge graph saved to: {path}")

# =============================================================================
# --- MAIN DEMO PIPELINE (UPDATED) ---
# =============================================================================
def run_demo_pipeline():
    """Executes the 5-step pipeline with Chain-of-Thought reasoning."""
    print("=" * 60); print("===  Knowledge Graph Construction with Chain-of-Thought  ==="); print("=" * 60)
    
    DEFAULT_DATA_DIR.mkdir(parents=True, exist_ok=True)
    text_path = DEFAULT_DATA_DIR / "sample_text.txt"
    text_content = "è‹¹æœå…¬å¸çš„CEOè’‚å§†Â·åº“å…‹æ˜¨å¤©åœ¨åŠ å·æ€»éƒ¨å®£å¸ƒäº†æ–°äº§å“iPhone 15ã€‚è¿™æ¬¾é©å‘½æ€§çš„æ™ºèƒ½æ‰‹æœºé‡‡ç”¨äº†å…ˆè¿›çš„A17èŠ¯ç‰‡ï¼Œæ‹¥æœ‰å“è¶Šçš„æ€§èƒ½å’Œå‡ºè‰²çš„æ‘„å½±èƒ½åŠ›ã€‚åº“å…‹å…ˆç”Ÿå¯¹è¿™æ¬¡å‘å¸ƒä¼šéå¸¸æ»¡æ„ï¼Œä»–è¡¨ç¤ºè¿™æ˜¯è‹¹æœå†å²ä¸Šæœ€é‡è¦çš„äº§å“ä¹‹ä¸€ã€‚ä¸æ­¤åŒæ—¶ï¼Œè‹¹æœçš„è‚¡ä»·åœ¨çº³æ–¯è¾¾å…‹äº¤æ˜“æ‰€ä¸Šæ¶¨äº†5%ã€‚"
    if not text_path.exists(): text_path.write_text(text_content, encoding="utf-8")
    
    text = text_path.read_text(encoding="utf-8")
    print(f"ğŸ“„ Processing text: \"{text}\"")
    
    try:
        extractor = InformationExtractor(get_llm_client(), DEFAULT_DATA_DIR)
    except (ValueError, RuntimeError) as e:
        print(f"\nâŒ CRITICAL ERROR: Could not initialize LLM client: {e}"); return

    # Log pipeline start
    extractor.logger.info(f"PIPELINE START - Processing text: {text}")

    # --- Step 1: Candidate Extraction ---
    print("\n[Step 1/5] Candidate Extraction...")
    candidate_entities, candidate_triples = extractor.extract_candidates(text)
    if not candidate_triples: print("  âŒ [Fatal] Halting."); return

    # --- Step 2: Schema Optimization ---
    print("\n[Step 2/5] Schema Optimization...")
    optimized_schema = extractor.optimize_ontology(candidate_entities, candidate_triples)
    if not optimized_schema: print("  âš ï¸ Schema optimization failed. Halting."); return
    print(f"    ğŸ“Š Optimized Schema Found: {optimized_schema}")

    # --- Step 3: Refinement & Relabeling ---
    print("\n[Step 3/5] Refinement & Relabeling...")
    refined_triples = extractor.refine_and_relabel(candidate_triples, optimized_schema)
    if not refined_triples: print("  âš ï¸ Refinement failed. Halting."); return
    print(f"    âœ¨ Found {len(refined_triples)} refined triples.")

    # --- Step 4: Role Analysis (Chain-of-Thought) ---
    print("\n[Step 4/5] Role Analysis (Chain-of-Thought)...")
    role_analysis_result = extractor.analyze_roles(text)
    print(f"    ğŸ§  LLM Reasoning: \"{role_analysis_result.get('reasoning', 'N/A')}\"")
    print(f"    ğŸ”¢ LLM Conclusion: {role_analysis_result.get('character_counts', 'N/A')}")

    # --- Step 5: Context-Aware Entity Linking ---
    print("\n[Step 5/5] Context-Aware Entity Linking...")
    # ä½¿ç”¨åœ¨æ­¥éª¤1ä¸­æå–åˆ°çš„æ‰€æœ‰å€™é€‰å®ä½“åç§°ï¼Œè€Œä¸æ˜¯ç²¾ç‚¼åçš„å®ä½“
    all_entity_names = list(candidate_entities.keys())
    entity_alias_map = extractor.link_entities(all_entity_names, role_analysis_result)
    print(f"    ğŸ”— Entity Alias Map Found: {entity_alias_map}")
    
    # --- Final Step: Build & Visualize ---
    print("\n[Final Step] Building and visualizing the final knowledge graph...")
    final_kg = KnowledgeGraph()
    final_kg.build_from_entities_and_triples(candidate_entities, refined_triples)
    
    # Log pre-linking state
    pre_linking_state = {
        "nodes_before_linking": list(final_kg.graph.nodes()),
        "edges_before_linking": [(u, v, data['predicate']) for u, v, data in final_kg.graph.edges(data=True)],
        "quality_before_linking": final_kg.assess_quality()
    }
    extractor._log_step("pre_linking_graph_state", pre_linking_state)
    
    if entity_alias_map: 
        final_kg.apply_entity_mapping(entity_alias_map)
        
        # Log post-linking state
        post_linking_state = {
            "nodes_after_linking": list(final_kg.graph.nodes()),
            "edges_after_linking": [(u, v, data['predicate']) for u, v, data in final_kg.graph.edges(data=True)],
            "quality_after_linking": final_kg.assess_quality(),
            "applied_mappings": entity_alias_map
        }
        extractor._log_step("post_linking_graph_state", post_linking_state)
    
    final_kg.print_quality_report("Final Fused Graph")
    
    # Log final summary
    final_summary = {
        "pipeline_completed": datetime.datetime.now().isoformat(),
        "steps_summary": {
            "step1_candidates": {"entities": len(candidate_entities), "triples": len(candidate_triples)},
            "step2_schema": optimized_schema,
            "step3_refined": {"triples": len(refined_triples)},
            "step4_analysis": role_analysis_result,
            "step5_linking": {"alias_map": entity_alias_map, "linking_pairs": len([k for k, v in entity_alias_map.items() if k != v])}
        },
        "final_graph": {
            "nodes": list(final_kg.graph.nodes()),
            "edges": [(u, v, data['predicate']) for u, v, data in final_kg.graph.edges(data=True)],
            "quality_assessment": final_kg.assess_quality()
        }
    }
    extractor._log_step("pipeline_final_summary", final_summary)
    
    print("\nğŸ”— Final Triples in Graph:")
    for sub, obj, data in final_kg.graph.edges(data=True):
        print(f"    - {sub} --[{data['predicate']}]--> {obj}")

    final_kg.visualize("Knowledge Graph with Chain-of-Thought", DEFAULT_DATA_DIR / "knowledge_graph.html")
    final_kg.save(DEFAULT_DATA_DIR / "knowledge_graph.json")
    print("\nâœ… Demo pipeline completed successfully!")

if __name__ == "__main__":
    run_demo_pipeline() 